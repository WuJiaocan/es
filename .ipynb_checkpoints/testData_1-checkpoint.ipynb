{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 调用百度接口返回分词结果，对分词结果进行处理，处理成每行一个字(单词)一个词性的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aip import AipNlp\n",
    "\n",
    "APP_ID = '********'\n",
    "API_KEY = '*********************'\n",
    "SECRET_KEY = '*************************'\n",
    "\n",
    "client = AipNlp(APP_ID, API_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改善QPS限制\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "SPLIT_LINE_MARKER = '！'\n",
    "SPLIT_LINE_MARKER_SIZE = 3\n",
    "\n",
    "# 拆分为句子\n",
    "# def sentencesMaker(html):\n",
    "\n",
    "#     import justext\n",
    "#     paragraphs = justext.justext(html, [])\n",
    "\n",
    "#     cache_sentences = ''\n",
    "#     sentences = []\n",
    "\n",
    "\n",
    "#     for p in paragraphs:\n",
    "#         sent = p.text.strip().replace('\\xa0', '').replace('\\u3000', '')\n",
    "#         sent = sent.encode('gb2312', 'ignore').decode('gb2312').encode('gbk', 'ignore').decode('gbk')\n",
    "#         if not sent:\n",
    "#             continue\n",
    "\n",
    "#         # 可能是含有名字，需要进一步处理\n",
    "#         if len(cache_sentences) < 5:\n",
    "#             cache_sentences += ' ' + sent\n",
    "\n",
    "#         else:\n",
    "#             sentences.append(cache_sentences.strip())\n",
    "#             cache_sentences = sent\n",
    "\n",
    "#     if not not cache_sentences:\n",
    "#         sentences.append(cache_sentences.strip())\n",
    "\n",
    "#     return sentences\n",
    "\n",
    "# 重新恢复句子\n",
    "def restoreSentences(text, only_per=False):  # 长度不超过3700字节的句子\n",
    "    restore_sentences = []\n",
    "    isSucc = False\n",
    "    if text is None:\n",
    "        return restore_sentences, isSucc\n",
    "\n",
    "    result = client.lexerCustom(text)\n",
    "\n",
    "    items = result.get('items', [])\n",
    "    items_size = len(items)\n",
    "\n",
    "    tries_limit = 3\n",
    "    tries_counter = 0\n",
    "    \n",
    "    while items_size == 0: # 分词结果为空\n",
    "        if len(text) != 0: # 但句子长度不为空\n",
    "            # 可能是qps限制\n",
    "            time.sleep(1)\n",
    "            result = client.lexerCustom(text)\n",
    "\n",
    "            items = result.get('items', [])\n",
    "            items_size = len(items)\n",
    "            isSucc=True\n",
    "\n",
    "        tries_counter += 1\n",
    "\n",
    "        if tries_counter >= tries_limit: # 分词尝试大于等于4次之后仍失败\n",
    "            print(f'error: 分词api请求失败多次！{text}')\n",
    "#             print('error: 分词api请求失败多次!')\n",
    "            return restore_sentences, isSucc\n",
    "\n",
    "    restore_idx = 0\n",
    "\n",
    "    last_restore_idx = 0\n",
    "    has_per = False\n",
    "\n",
    "    while restore_idx < items_size: # 对每个分词结果进行整理 \n",
    "        # 分词不是拼接符\"!!!\"\n",
    "        while restore_idx < items_size and items[restore_idx]['item'] != SPLIT_LINE_MARKER:\n",
    "            item = items[restore_idx] # 先把第一个分词的结果(dict)赋值给item，item整理之后再直接赋给原items[]\n",
    "            # TODO 剔除机构中的不合法字符\n",
    "            format_pos = item['pos']\n",
    "            \n",
    "            # 对非ne标识的分词不做处理\n",
    "            \n",
    "            if item['ne'].startswith('ORG'): # 如果该分词的ne是ORG\n",
    "                invalid_orgs = ['公司']\n",
    "                item['item'] = item['item'].replace('&', '')\n",
    "                if item['item'] in invalid_orgs:\n",
    "                    format_pos = 'n' # 普通名词\n",
    "                else:\n",
    "                    format_pos = 'nt'  # 机构团体名\n",
    "\n",
    "            elif item['ne'] == 'PER': # 如果该分词的ne是PER\n",
    "                format_pos = 'nr' # 人名\n",
    "\n",
    "            elif item['ne'] == 'TITLE': # 如果该分词的ne是TITLE（定制）\n",
    "                format_pos = 'ti' # 职称\n",
    "\n",
    "            elif item['ne'] == 'LOC': # 如果该分词的ne是LOC\n",
    "                format_pos = 'ns'  # 地名\n",
    "\n",
    "            elif item['ne'] == 'TIME': # 如果该分词的ne是TIME\n",
    "                format_pos = 't' # 时间名词\n",
    "                \n",
    "                \n",
    "\n",
    "            if format_pos == '': #如果pos为空，即是其他非上述的ne标识，将pos置为\"xx\"\n",
    "                format_pos = 'xx'\n",
    "\n",
    "            elif format_pos == 'nr':\n",
    "                # 过滤先生或者女士之类的名称\n",
    "                name = re.sub(r'((先生)|(小姐)|(阿姨)|(叔叔)|(女士)|(同志)|总)$', '', item['item'])\n",
    "\n",
    "                if len(name) >= 2:\n",
    "                    invalid_names = {\n",
    "                        '区块链': 'n'\n",
    "                    }\n",
    "\n",
    "                    if name not in invalid_names:\n",
    "                        has_per = True\n",
    "                        item['item'] = name\n",
    "                    else:\n",
    "                        format_pos = invalid_names[name]\n",
    "\n",
    "                else: # 剔除称谓之后的name长度如果小于2，就不是nr，设为n\n",
    "\n",
    "                     format_pos = 'n'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            item['pos'] = format_pos # 处理之后的pos赋值给原分词的pos\n",
    "\n",
    "            # 删除无用信息\n",
    "            item.pop('basic_words')\n",
    "            item.pop('formal')\n",
    "            item.pop('byte_length')\n",
    "            item.pop('byte_offset')\n",
    "            item.pop('loc_details')\n",
    "            item.pop('ne')\n",
    "            item.pop('uri')\n",
    "\n",
    "            items[restore_idx] = item # 处理之后的分词结果赋给原分词结果\n",
    "            restore_idx += 1 # 继续下一个item\n",
    "\n",
    "            \n",
    "        # 若遇到了拼接符\"!!!\"或对所有非拼接符的分词结果处理完毕    \n",
    "        if restore_idx + SPLIT_LINE_MARKER_SIZE - 1 < items_size: # 如果不是句末的最后一个拼接符\n",
    "            needCut = True # 将修改之后的分词结果仍以一个sent为单位cut\n",
    "            for i in range(SPLIT_LINE_MARKER_SIZE - 1): # 再次判断后一位是否是拼接符，如不是，不需要cut\n",
    "                if items[restore_idx + i + 1]['item'] != SPLIT_LINE_MARKER:\n",
    "                    needCut = False\n",
    "                    break\n",
    "\n",
    "            if needCut:\n",
    "                ed = max(restore_idx, 0)\n",
    "\n",
    "                sentence_items = items[last_restore_idx:ed] # 切句子，[0:restore_idx]\n",
    "                if len(sentence_items) != 0 and (has_per or not only_per):\n",
    "                    # print('per:', sentence_items)\n",
    "                    restore_sentences.append(sentence_items)\n",
    "\n",
    "                next_st = min(ed + SPLIT_LINE_MARKER_SIZE, items_size)\n",
    "                last_restore_idx = next_st\n",
    "\n",
    "                restore_idx += SPLIT_LINE_MARKER_SIZE\n",
    "            else:\n",
    "                restore_idx += 1\n",
    "\n",
    "        else: # 句末最后一个拼接符\n",
    "            ed = max(restore_idx, 0)\n",
    "            sentence_items = items[last_restore_idx:ed]\n",
    "            if len(sentence_items) != 0 and (has_per or not only_per):\n",
    "\n",
    "                restore_sentences.append(sentence_items)\n",
    "\n",
    "            restore_idx = items_size\n",
    "\n",
    "        has_per = False\n",
    "\n",
    "    # print(restore_sentences)\n",
    "    return restore_sentences, isSucc\n",
    "\n",
    "\n",
    "# 解析并标注HTML\n",
    "def posHtml(sentences, only_per=False):\n",
    "#     sentences = sentencesMaker(html) # 将带有html标签的段落整理成一个list,去除标签的文字段落。\n",
    "\n",
    "\n",
    "    cut_str = '' \n",
    "\n",
    "    pos_sentences = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if not sent:  # 若句子为空，跳出循环\n",
    "            continue\n",
    "        if not cut_str: # 若cut_str为空，即第一句之前，把第一句话赋值给tmp_str\n",
    "            tmp_str = sent\n",
    "        else: # 若cut_str里有句子，将后续句子拼接，以“!!!”作为拼接符，再赋值给tmp_str\n",
    "            tmp_str = cut_str + SPLIT_LINE_MARKER * SPLIT_LINE_MARKER_SIZE + sent\n",
    "        if sys.getsizeof(tmp_str) < 3700: # 拼接之后的句子小于3700字节\n",
    "            cut_str = tmp_str # tmp_str赋值给cut_str，继续拼接sent\n",
    "        else:\n",
    "            try:\n",
    "                if cut_str:\n",
    "                    time.sleep(0.5) # 拼接之后的tmp_str若大于3700字节，取未拼接该sent之前的cut_str拿来分词\n",
    "                    sents, issucc = restoreSentences(cut_str, only_per)\n",
    "                    pos_sentences += sents\n",
    "    #                 if not issucc:\n",
    "    #                     print(html)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('error: ', sent)\n",
    "\n",
    "            cut_str = sent # 把刚才没有拼接成功的sent重新赋给cut_str\n",
    "\n",
    "\n",
    "    if not not cut_str: # 最后一句sent\n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            if cut_str:\n",
    "                time.sleep(0.5) # 拼接之后的tmp_str若大于3700字节，取未拼接该sent之前的cut_str拿来分词\n",
    "                sents, issucc = restoreSentences(cut_str, only_per)\n",
    "                pos_sentences += sents\n",
    "#                 if not issucc:\n",
    "#                     print(html)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('error: ', sent)\n",
    "    return pos_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 分词api请求失败多次！link: http://cvpr2018.thecvf.com/CVPR 2018论文列表CVPR 2018 Accepted PapersSingle-Shot Refinement Neural Network for Object DetectionVideo Captioning via Hierarchical Reinforcement LearningDensePose: Multi-Person Dense Human Pose Estimation In The WildDensePose: Multi-Person Dense Human Pose Estimation In The WildFrustum PointNets for 3D Object Detection from RGB-D DataTips and Tricks for Visual Question Answering: Learnings from the 2017 ChallengeRethinking the Faster R-CNN Architecture for Temporal Action LocalizationShape from Shading through Shape EvolutionShape from Shading through Shape EvolutionA High-Quality Denoising Dataset for Smartphone CamerasImproving Color Reproduction Accuracy in the Camera Imaging PipelineEnd-to-End Dense Video Captioning with Masked TransformerEnd-to-End Dense Video Captioning with Masked TransformerpOSE: Pseudo Object Space Error for Initialization-Free Bundle AdjustmentLearning to Segment Every ThingDensity-aware Single Image De-raining using a Multi-stream Dense NetworkDensely Connected Pyramid Dehazing NetworkEmbodied Question AnsweringTieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-raysTieNet: Text-Image Embedding Network for Common Thorax Disease Classification and Reporting in Chest X-raysTowards Open-Set Identity Preserving Face SynthesisBaseline Desensitizing In Translation AveragingLearning from the Deep: A Revised Underwater Image Formation ModelContext Encoding for Semantic SegmentationContext Encoding for Semantic SegmentationDeep Texture Manifold for Ground Terrain RecognitionDS*: Tighter Lifting-Free Convex Relaxations for Quadratic Matching ProblemsSparse, Smart Contours to Represent and Edit ImagesEvery Smile is Unique: Landmark-guided Diverse Smile GenerationGenerative Non-Rigid Shape Completion with Graph Convolutional AutoencodersLearning a Discriminative Prior for Blind Image DeblurringAttentional ShapeContextNet for Point Cloud RecognitionLearning Superpixels with Segmentation-Aware Affinity LossReal-World Repetition Estimation by Div, Grad and CurlReal-World Repetition Estimation by Div, Grad and CurlRecurrent Saliency Transformation Network: Incorporating Multi-Stage Visual Cues for Small Organ SegmentationMegaDepth: Learning Single-View Depth Prediction from Internet PhotosLearning Intrinsic Image Decomposition from Watching the WorldLearning Intrinsic Image Decomposition from Watching the WorldDon't Just Assume; Look and Answer: Overcoming Priors for Visual Question AnsweringHuman-centric Indoor Scene Synthesis Using Stochastic GrammarLearning by Asking QuestionsInstance Embedding Transfer to Unsupervised Video Object SegmentationDetect-and-Track: Efficient Pose Estimation in VideosSelf-Supervised Adversarial Hashing Networks for Cross-Modal RetrievalGuided Proofreading of Automatic Segmentations for ConnectomicsAugmented Skeleton Space Transfer for Depth-based Hand Pose EstimationAugmented Skeleton Space Transfer for Depth-based Hand Pose EstimationContext-aware Synthesis for Video Frame Interpolation2D/3D Pose Estimation and Action Recognition using Multitask Deep LearningNAG: Network for Adversary GenerationLiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow EstimationLiteFlowNet: A Lightweight Convolutional Neural Network for Optical Flow EstimationAvatar-Net: Multi-scale Zero-shot Style Transfer by Feature DecorationMulti-view Harmonized Bilinear Network for 3D Object RecognitionMulti-view Harmonized Bilinear Network for 3D Object RecognitionTangent Convolutions for Dense Prediction in 3DTangent Convolutions for Dense Prediction in 3DSemi-parametric Image SynthesisSemi-parametric Image SynthesisInteractive Image Segmentation with Latent Diversity3D Hand Pose Estimation: From Current Achievements to Future Goals3D Hand Pose Estimation: From Current Achievements to Future GoalsW2F: A Weakly-Supervised to Fully-Supervised Framework for Object DetectionBlockDrop: Dynamic Inference Paths in Residual NetworksBlockDrop: Dynamic Inference Paths in Residual NetworksMapNet: Geometry-Aware Learning of Maps for Camera LocalizationMapNet: Geometry-Aware Learning of Maps for Camera LocalizationBPGrad: Towards Global Optimality in Deep Learning via Branch and PruningSalient Object Detection Driven by Fixation Prediction3D Object Detection with Latent Support SurfacesPractical Block-wise Neural Network Architecture GenerationPractical Block-wise Neural Network Architecture GenerationGlimpse Clouds: Human Activity Recognition from Unstructured Feature PointsAre You Talking to Me? Reasoned Visual Dialog Generation through Adversarial LearningAre You Talking to Me? Reasoned Visual Dialog Generation through Adversarial LearningVisual Grounding via Accumulated AttentionSupervision-by-Registration: An Unsupervised Approach to Improve the Precision of Facial Landmark DetectorsISTA-Net: Interpretable Optimization-Inspired Deep Network for Image Compressive SensingPerturbative Neural Networks: Rethinking Convolution in CNNsNonlinear 3D Face Morphable ModelNonlinear 3D Face Morphable ModelNeural Baby TalkNeural Baby TalkTowards Pose Invariant Face Recognition in the WildMoNet: Deep Motion Exploitation for Video Object SegmentationExploring Disentangled Feature Representation Beyond Face IdentificationTowards Effective Low-bitwidth Convolutional Neural NetworksParallel Attention: A Unified Framework for Visual Object Discovery through Dialogs and QueriesLearning Facial Action Units from Web Images with Scalable Weakly Supervised ClusteringFew-Shot Image Recognition by Predicting Parameters from ActivationsFew-Shot Image Recognition by Predicting Parameters from ActivationsSingle-Shot Object Detection with Enriched SemanticsUnifying Identification and Context Learning for Person RecognitionSeparating Self-Expression and Visual Content in Hashtag SupervisionMulti-Cue Correlation Filters for Robust Visual TrackingBeyond Trade-off: Accelerate FCN-based Face Detection with Higher AccuracyOn the Robustness of Semantic Segmentation Models to Adversarial AttacksPWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost VolumePWC-Net: CNNs for Optical Flow Using Pyramid, Warping, and Cost VolumeIlluminant Spectra-based Source Separation Using Flash PhotographyIlluminant Spectra-based Source Separation Using Flash PhotographyTracking Multiple Objects Outside the Line of Sight using Speckle ImagingTracking Multiple Objects Outside the Line of Sight using Speckle ImagingImproved Human Pose Estimation through Adversarial Data AugmentationGenerative Adversarial Learning Towards Fast Weakly Supervised DetectionAudio to Body DynamicsAudio to Body DynamicsThe Unreasonable Effectiveness of Deep Features as a Perceptual MetricFrame-Recurrent Video Super-ResolutionDeep Mutual LearningReal-world Anomaly Detection in Surveillance VideosSoccer on Your TabletopDiversity Regularized Spatiotemporal Attention for Video-based Person Re-identificationHashGAN: Deep Learning to Hash with Pair Conditional Wasserstein GANExcitation Backprop for RNNsDynamic-Structured Semantic Propagation NetworkSuper SloMo: High Quality Estimation of Multiple Intermediate Frames for Video InterpolationSuper SloMo: High Quality Estimation of Multiple Intermediate Frames for Video InterpolationSPLATNet: Sparse Lattice Networks for Point Cloud ProcessingSPLATNet: Sparse Lattice Networks for Point Cloud ProcessingVideo Representation Learning Using Discriminative PoolingAttend and Interact: Higher-Order Object Interactions for Video UnderstandingHuman Pose Estimation with Parsing Induced Learner4D Human Body Correspondences from Panoramic Depth MapsRecognizing Human Actions as Evolution of Pose Estimation MapsGraphBit: Bitwise Interaction Mining via Deep Reinforcement LearningDeep Adversarial Metric LearningDeep Adversarial Metric LearningRevisiting Video Saliency: A Large-scale Benchmark and a New ModelGraph-Cut RANSACFive-point Fundamental Matrix Estimation for Uncalibrated CamerasHashing as Tie-Aware Learning to RankOptimizing Local Feature Descriptors for Nearest Neighbor MatchingTotal Capture: A 3D Deformation Model for Tracking Faces, Hands, and BodiesTotal Capture: A 3D Deformation Model for Tracking Faces, Hands, and BodiesConsensus Maximization for Semantic Region CorrespondencesConsensus Maximization for Semantic Region CorrespondencesST-GAN: Spatial Transformer Generative Adversarial Networks for Image CompositingMotion-Guided Cascaded Refinement Network for Video Object SegmentationZigzag Learning for Weakly Supervised Object DetectionLook, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative ModelsLook, Imagine and Match: Improving Textual-Visual Cross-Modal Retrieval with Generative ModelsVITON: An Image-based Virtual Try-on NetworkVITON: An Image-based Virtual Try-on NetworkCross-Domain Self-supervised Multi-task Feature Learning Using Synthetic Game ImageryLayoutNet: Reconstructing the 3D Room Layout from a Single RGB ImageThoracic Disease Identification and Localization with Limited SupervisionStochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional NetworksLearning Pixel-level Semantic Affinity with Image-level Supervision for Weakly Supervised Semantic SegmentationDeep End-to-End Time-of-Flight ImagingFast and Accurate Online Video Object Segmentation via Tracking PartsFast and Accurate Online Video Object Segmentation via Tracking PartsMin-Entropy Latent Model for Weakly Supervised Object DetectionFuture Frame Prediction for Anomaly Detection A New BaselineFace Aging with Identity-Preserved Conditional Generative Adversarial NetworksLearning to Compare: Relation Network for Few-Shot LearningDeep Layer AggregationDeep Layer AggregationStyle Aggregated Network for Facial Landmark DetectionM3: Multimodal Memory Modelling for Video CaptioningM3: Multimodal Memory Modelling for Video CaptioningClassification Driven Dynamic Image EnhancementGenerative Image Inpainting with Contextual AttentionIterative Visual Reasoning Beyond ConvolutionsIterative Visual Reasoning Beyond ConvolutionsDual Attention Matching Network for Context-Aware Feature Sequence based Person Re-IdentificationTextbook Question Answering under Teacher Guidance with Memory NetworksTextbook Question Answering under Teacher Guidance with Memory NetworksMulti-Level Factorisation Net for Person Re-IdentificationFunctional Map of the WorldFunctional Map of the WorldA Two-Step Disentanglement MethodTowards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root NormalizationCan Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?Left-Right Comparative Recurrent Model for Stereo MatchingLeft-Right Comparative Recurrent Model for Stereo MatchingAnalytic Expressions for Probabilistic Moments of PL-DNN with Gaussian InputAnalytic Expressions for Probabilistic Moments of PL-DNN with Gaussian InputZero-Shot Sketch-Image HashingZero-Shot Sketch-Image HashingInterpretable Convolutional Neural NetworksInterpretable Convolutional Neural NetworksReconstructing Thin Structures of Manifold Surfaces by Integrating Spatial CurvesEnhancing the Spatial Resolution of Stereo Images using a Parallax PriorAnticipating Traffic Accidents with Adaptive Loss and Large-scale Incident DBGenerating Synthetic X-ray Images of a Person from the Surface GeometryGenerating Synthetic X-ray Images of a Person from the Surface GeometryAttentive Fashion Grammar Network for Fashion Landmark Detection and Clothing Category ClassificationUnsupervised CCADiscovering Point Lights with Intensity Distance FieldsUniversal Denoising Networks : A Novel CNN-based Network Architecture for Image DenoisingEasy Identification from Better Constraints: Multi-Shot Person Re-Identification from Reference ConstraintsRecurrent Pixel Embedding for Instance GroupingRecurrent Pixel Embedding for Instance GroupingRecurrent Scene Parsing with Perspective Understanding in the LoopLearning to Hash by Discrepancy MinimizationFast End-to-End Trainable Guided FilterDisentangling Structure and Aesthetics for Content-aware Image CompletionAn Analysis of Scale Invariance in Object Detection - SNIPAn Analysis of Scale Invariance in Object Detection - SNIPCSGNet: Neural Shape Parser for Constructive Solid GeometryFinding Tiny Faces in the Wild with Generative Adversarial NetworkFinding Tiny Faces in the Wild with Generative Adversarial NetworkSSNet: Scale Selection Network for Online 3D Action PredictionSSNet: Scale Selection Network for Online 3D Action PredictionIntegrated facial landmark localization and super-resolution of real-world very low resolution faces in arbitrary poses with GANsIntegrated facial landmark localization and super-resolution of real-world very low resolution faces in arbitrary poses with GANsThe Best of Both Worlds: Combining CNNs and Geometric Constraints for Hierarchical Motion SegmentationIn-Place Activated BatchNorm for Memory-Optimized Training of DNNsWing Loss for Robust Facial Landmark Localisation with Convolutional Neural NetworksDeep Cross-media Knowledge TransferDeep Cross-media Knowledge TransferCoupled End-to-end Transfer Learning with Generalized Fisher InformationKnowledge Aided Consistency for Weakly Supervised Phrase GroundingViewpoint-aware Attentive Multi-view Inference for Vehicle Re-identificationMatNet: Modular Attention Network for Referring Expression ComprehensionCBMV: A Coalesced Bidirectional Matching Volume for Disparity EstimationNISP: Pruning Networks using Neuron Importance Score PropagationNISP: Pruning Networks using Neuron Importance Score PropagationWho Let The Dogs Out? Modeling Dog Behavior From Visual DataEfficient Video Object Segmentation via Network ModulationLearning Deep Models for Face Anti-Spoofing: Binary or Auxiliary SupervisionFeedback-prop: Convolutional Neural Network Inference under Partial EvidenceA Memory Network Approach for Story-based Temporal Summarization of 360?VideosImproving Occlusion and Hard Negative Handling for Single-Stage Object DetectorsUV-GAN: Adversarial Facial UV Map Completion for Pose-invariant Face RecognitionLearning a Toolchain for Image RestorationLearning a Toolchain for Image RestorationLearning to Act Properly: Predicting and Explaining Affordances from ImagesLearning a Discriminative Feature Network for Semantic SegmentationOptimizing Video Object Detection via a Scale-Time LatticeShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile DevicesCascaded Pyramid Network for Multi-Person Pose EstimationSeeing Temporal Modulation of Lights from Standard CamerasPoint-wise Convolutional Neural NetworksFine-grained Video Captioning for Sports NarrativeFine-grained Video Captioning for Sports NarrativeDense 3D Regression for Hand Pose EstimationMissing Slice Recovery for Tensors Using a Low-rank Model in Embedded SpaceLearning Convolutional Networks for Content-weighted Image CompressionLearning Attentions: Residual Attentional Siamese Network for High Performance Online Visual TrackingDeep Cost-Sensitive and Order-Preserving Feature Learning for Cross-Population Age EstimationFirst-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose AnnotationsHand PointNet: 3D Hand Pose Estimation using Point SetsHand PointNet: 3D Hand Pose Estimation using Point SetsRecovering Realistic Texture in Image Super-resolution by Spatial Feature ModulationCube Padding for Weakly-Supervised Saliency Prediction in 360$^{circ}$ VideosA Face to Face Neural Conversation ModelSurfConv: Bridging 3D and 2D Convolution for RGBD ImagesDynamic Video Segmentation NetworkMultiple Granularity Group Interaction PredictionVisual Question Reasoning on General Dependency TreeVisual Question Reasoning on General Dependency TreeFrom Lifestyle VLOGs to Everyday InteractionsCOCO-Stuff: Thing and Stuff Classes in ContextGANerated Hands for Real-Time 3D Hand Tracking from Monocular RGBGANerated Hands for Real-Time 3D Hand Tracking from Monocular RGBNon-local Neural NetworksZero-shot Recognition via Semantic Embeddings and Knowledge GraphsTaskonomy: Disentangling Task Transfer LearningTaskonomy: Disentangling Task Transfer LearningEmbodied Real-World Active PerceptionEmbodied Real-World Active PerceptionSfSNet : Learning Shape, Reflectance and Illuminance of Faces `in the wild'SfSNet : Learning Shape, Reflectance and Illuminance of Faces `in the wild'End-to-end Recovery of Human Shape and PoseFactoring Shape, Pose, and Layout from the 2D Image of a 3D SceneMulti-view Consistency as Supervisory Signal for Learning Shape and Pose PredictionA Fast Resection-Intersection Method for the Known Rotation ProblemImage Generation from Scene GraphsWhat Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and DatasetsWhat Makes a Video a Video: Analyzing Temporal Information in Video Understanding Models and DatasetsPointFusion: Deep Sensor Fusion for 3D Bounding Box EstimationHigh-Resolution Image Synthesis and Semantic Manipulation with Conditional GANsHigh-Resolution Image Synthesis and Semantic Manipulation with Conditional GANsSocial GAN: Socially Acceptable Trajectories with Generative Adversarial NetworksQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only InferenceQuantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only InferenceFinding It\": Weakly-Supervised Reference-Aware Visual Grounding in Instructional Video\"Finding It\": Weakly-Supervised Reference-Aware Visual Grounding in Instructional Video\"Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal PatternsKernelized Subspace Pooling for Deep Local DescriptorsVideo Rain Removal By Multiscale Convolutional Sparse CodingLearning from Millions of 3D Scans for Large-scale 3D Face RecognitionReferring RelationshipsImproving Object Localization with Fitness NMS and Bounded IoU LossUnsupervised Feature Learning via Non-Parametric Instance-level DiscriminationUnsupervised Feature Learning via Non-Parametric Instance-level DiscriminationCVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-LocalizationCVM-Net: Cross-View Matching Network for Image-Based Ground-to-Aerial Geo-LocalizationVisual Question Generation as Dual Task of Visual Question AnsweringVisual Question Generation as Dual Task of Visual Question AnsweringRevisiting Dilated Convolution: A Simple Approach for Weakly- and Semi- Supervised Semantic SegmentationRevisiting Dilated Convolution: A Simple Approach for Weakly- and Semi- Supervised Semantic SegmentationLearning Dual Convolutional Neural Networks for Low-Level VisionDeep Video Super-Resolution Network Using Dynamic Upsampling Filters Without Explicit Motion CompensationMegDet: A Large Mini-Batch Object DetectorMegDet: A Large Mini-Batch Object DetectorAttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial NetworksTOM-Net: Learning Transparent Object Matting from a Single ImageTOM-Net: Learning Transparent Object Matting from a Single ImageEnd-to-End Deep Kronecker-Product Matching for Person Re-identificationSemantic Visual LocalizationJoint Cuts and Matching of Partitions in One GraphBenchmarking 6DOF Outdoor Visual Localization in Changing ConditionsBenchmarking 6DOF Outdoor Visual Localization in Changing ConditionsCrowd Counting via Adversarial Cross-Scale Consistency PursuitDeep Group-shuffling Random Walk for Person Re-identificationLearning to Detect Features in Texture ImagesLearning to Detect Features in Texture ImagesTransferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-IdentificationCarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of VehiclesContext-aware Deep Feature Compression for High-speed Visual TrackingDeep Material-aware Cross-spectral Stereo MatchingDeep Extreme Cut: From Extreme Points to Object SegmentationLabel Denoising Adversarial Network (LDAN) for Inverse Lighting of Face ImagesLabel Denoising Adversarial Network (LDAN) for Inverse Lighting of Face ImagesHarmonious Attention Network for Person Re-IdenticationUnsupervised Deep Generative Adversarial Hashing NetworkUnsupervised Deep Generative Adversarial Hashing NetworkPseudo-Mask Augmented Object DetectionLSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)LSTM stack-based Neural Multi-sequence Alignment TeCHnique (NeuMATCH)Adversarial Complementary Learning for Weakly Supervised Object LocalizationUnsupervised Discovery of Object Landmarks as Structural RepresentationsUnsupervised Discovery of Object Landmarks as Structural RepresentationsDeLS-3D: Deep Localization and Segmentation with a 3D Semantic MapMonocular Relative Depth Perception with Web Stereo Data SupervisionImage-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identificationObjects as context for detecting their semantic partsCamera Style Adaptation for Person Re-identificationConditional Generative Adversarial Network for Structured Domain AdaptationRotation-sensitive Regression for Oriented Scene Text DetectionResidual Parameter Transfer for Deep Domain AdaptationSGPN: Similarity Group Proposal Network for 3D Point Cloud Instance SegmentationSGPN: Similarity Group Proposal Network for 3D Point Cloud Instance SegmentationWeakly Supervised Instance Segmentation using Class Peak ResponseWeakly Supervised Instance Segmentation using Class Peak ResponseRobust Facial Landmark Detection via a Fully-Convolutional Local-Global Context NetworkRotation Averaging and Strong DualityRotation Averaging and Strong DualityPackNet: Adding Multiple Tasks to a Single Network by Iterative PruningIm2Flow: Motion Hallucination from Static Images for Action RecognitionIm2Flow: Motion Hallucination from Static Images for Action RecognitionFeature Quantization for Defending Against Distortion of ImagesEnd-to-end weakly-supervised semantic alignmentPointGrid: A Deep Network for 3D Shape UnderstandingPointGrid: A Deep Network for 3D Shape UnderstandingImagine it for me: Generative Adversarial Approach for Zero-Shot Learning from Noisy TextsA Minimalist Approach to Type-Agnostic Detection of Quadrics in Point CloudsA Benchmark for Articulated Human Pose Estimation and TrackingBoosting Self-Supervised Learning via Knowledge TransferPPFNet: Global Context Aware Local Features for Robust 3D Point MatchingPPFNet: Global Context Aware Local Features for Robust 3D Point MatchingVision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environmentsVision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environmentsFast Video Object Segmentation by Reference-Guided Mask PropagationFast Video Object Segmentation by Reference-Guided Mask PropagationSuper-Resolving Very Low-Resolution Face Images with Supplementary AttributesVideo Person Re-identification with Competitive Snippet-similarity Aggregation and Co-attentive Snippet EmbeddingOne-shot Action Localization by Sequence Matching NetworkEfficient Subpixel Refinement with Symbolic Linear PredictorsDistort-and-Recover: Color Enhancement using Deep Reinforcement LearningGroup Consistent Similarity Learning via Deep CRFs for Person Re-IdentificationGroup Consistent Similarity Learning via Deep CRFs for Person Re-IdentificationSingle Image Reflection Separation with Perceptual LossesAVA: A Video Dataset of Spatio-temporally Localized Atomic Visual ActionsAVA: A Video Dataset of Spatio-temporally Localized Atomic Visual ActionsRecognize Actions by Disentangling Components of DynamicsZoom and Learn: Generalizing Deep Stereo Matching to Novel DomainsAttention-aware Compositional Network for Person Re-IdentificationHATS: Histograms of Averaged Time Surfaces for Robust Event-based Object ClassificationMask-guided Contrastive Attention Model for Person Re-IdentificationPose-Guided Photorealistic Face RotationPose-Guided Photorealistic Face RotationAutomatic 3D Indoor Scene Modeling from Single PanoramaAutomatic 3D Indoor Scene Modeling from Single PanoramaSobolevFusion: 3D Reconstruction of Scenes Undergoing Free Non-rigid MotionSobolevFusion: 3D Reconstruction of Scenes Undergoing Free Non-rigid MotionA Biresolution Spectral framework for Product QuantizationDynamic Zoom-in Network for Fast Object Detection in Large ImagesOn the Importance of Label Quality for Semantic SegmentationEPINET: A Fully-Convolutional Neural Network for Light Field Depth Estimation by Using Epipolar GeometryA Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-RankingErase or Fill? Deep Joint Recurrent Rain Removal and Reconstruction in VideosScalable and Effective Deep CCA via Soft DecorrelationHigh-order tensor regularization with application to attribute ranking3D-RCNN: Instance-level 3D Scene Understanding via Render-and-Compare3D-RCNN: Instance-level 3D Scene Understanding via Render-and-CompareFoldingNet: Interpretable Unsupervised Learning on 3D Point CloudsFoldingNet: Interpretable Unsupervised Learning on 3D Point CloudsDefocus Blur Detection via Multi-Stream Bottom-Top-Bottom Fully Convolutional NetworkDecorrelated Batch NormalizationUnsupervised Textual Grounding: Linking Words to Image ConceptsUnsupervised Textual Grounding: Linking Words to Image ConceptsScale-recurrent Network for Deep Image DeblurringLow-Shot Recognition with Imprinted WeightsBottom-Up and Top-Down Attention for Image Captioning and Visual Question AnsweringBottom-Up and Top-Down Attention for Image Captioning and Visual Question AnsweringCross-Domain Weakly-Supervised Object Detection through Progressive Domain AdaptationFacelet-Bank for Fast Portrait ManipulationDuplex Generative Adversarial Network for Unsupervised Domain AdaptationQuantization of Fully Convolutional Networks for Accurate Biomedical Image SegmentationReal-Time Rotation-Invariant Face Detection with Progressive Calibration NetworksStructure Preserving Video PredictionTagging Like Humans: Diverse and Distinct Image AnnotationLearning to Sketch with Shortcut Cycle ConsistencyGroupCap: Group-based Image Captioning with Structured Relevance and Diversity ConstraintsDynamic Scene Deblurring Using Spatially Variant Recurrent Neural NetworksDynamic Scene Deblurring Using Spatially Variant Recurrent Neural NetworksHyperparameter Optimization for Tracking with Continuous Deep Q-LearningDeep Unsupervised Saliency Detection: A Multiple Noisy Labeling PerspectiveDeep Unsupervised Saliency Detection: A Multiple Noisy Labeling PerspectiveNeuralNetwork-Viterbi: A Framework for Weakly Supervised Video LearningNeuralNetwork-Viterbi: A Framework for Weakly Supervised Video LearningDetecting and Recognizing Human-Object InteractionsDetecting and Recognizing Human-Object InteractionsAugmenting Crowd-Sourced 3D Reconstructions using Semantic DetectionsVisual Relationship Learning with a Factorization-based PriorRe-weighted Adversarial Adaptation Network for Unsupervised Domain AdaptationFlow Guided Recurrent Neural Encoder for Video Salient Object DetectionDisentangling 3D Pose in A Dendritic CNN for Unconstrained 2D Face AlignmentProgressive Attention Guided Recurrent Network for Salient Object DetectionAnswer with Grounding Snippets: Focal Visual-Text Attention for Visual Question AnsweringAnswer with Grounding Snippets: Focal Visual-Text Attention for Visual Question AnsweringUnsupervised Learning of Depth and Egomotion from Monocular Video Using 3D Geometric ConstraintsRepulsion Loss: Detecting Pedestrians in a CrowdPU-Net: Point Cloud Upsampling NetworkVideo Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRFVideo Object Segmentation via Inference in A CNN-Based Higher-Order Spatio-Temporal MRFPiCANet: Learning Pixel-wise Contextual Attention for Saliency DetectionGated Fusion Network for Single Image DehazingInterleaved Structured Sparse Convolutional Neural NetworksInterleaved Structured Sparse Convolutional Neural NetworksWhere and Why Are They Looking? Jointly Inferring Human Attention and Intentions in Complex TasksEnd-to-end Flow Correlation Tracking with Spatial-temporal AttentionLeft/Right Asymmetric Layer Skippable NetworksContext Contrasted Feature and Gated Multi-scale Aggregation for Scene SegmentationContext Contrasted Feature and Gated Multi-scale Aggregation for Scene SegmentationVITAL: VIsual Tracking via Adversarial LearningVITAL: VIsual Tracking via Adversarial LearningRotationNet: Joint Object Categorization and Pose Estimation Using Multiviews from Unsupervised ViewpointsAction Sets: Weakly Supervised Action Segmentation without Ordering ConstraintsAction Sets: Weakly Supervised Action Segmentation without Ordering ConstraintsSqueeze-and-Excitation NetworksSqueeze-and-Excitation NetworksEdit Probability for Scene Text RecognitionBidirectional Attentive Fusion with Context Gating for Dense Video CaptioningBidirectional Attentive Fusion with Context Gating for Dense Video CaptioningExploit the Unknown Gradually:~ One-Shot Video-Based Person Re-Identification by Stepwise LearningLearning to Localize Sound Source in Visual ScenesDynamic Few-Shot Visual Learning without ForgettingWeakly-Supervised Semantic Segmentation by Iteratively Mining Common Object FeaturesSINT++: Robust Visual Tracking via Adversarial Hard Positive GenerationReal-Time Monocular Depth Estimation using Synthetic Data with Domain Adaptation via Image Style TransferFast and Accurate Single Image Super-Resolution via Information Distillation NetworkLow-Latency Video Semantic SegmentationLow-Latency Video Semantic SegmentationDomain Adaptive Faster R-CNN for Object Detection in the WildDoubleFusion: Real-time Capture of Human Performance with Inner Body Shape from a Single Depth SensorDoubleFusion: Real-time Capture of Human Performance with Inner Body Shape from a Single Depth SensorLean Multiclass CrowdsourcingLean Multiclass CrowdsourcingTell Me Where To Look: Guided Attention Inference NetworkTell Me Where To Look: Guided Attention Inference NetworkResidual Dense Network for Image Super-ResolutionResidual Dense Network for Image Super-ResolutionLook at Boundary: A Boundary-Aware Face Alignment AlgorithmImagination-IQA: No-reference Image Quality Assessment via Adversarial LearningMemory Matching Networks for One-Shot Image Recognition3D Human Pose Estimation in the Wild by Adversarial LearningUnsupervised Training for 3D Morphable Model RegressionUnsupervised Training for 3D Morphable Model RegressionScalable Dense Non-rigid Structure-from-Motion: A Grassmannian PerspectiveIQA: Visual Question Answering in Interactive EnvironmentsLearning Spatial-Temporal Regularized Correlation Filters for Visual TrackingLow-shot Learning from Imaginary DataLow-shot Learning from Imaginary DataDeep Regression Forests for Age EstimationPartial Transfer Learning with Selective Adversarial NetworksPartial Transfer Learning with Selective Adversarial NetworksA Bi-directional Message Passing Model for Salient Object DetectionTransductive Unbiased Embedding for Zero-Shot LearningScale-Transferrable Object DetectionCrowd Counting with Deep Negative Correlation LearningDeep Cauchy Hashing for Hamming Space RetrievalDemo2Vec: Reasoning Object Affordances from Online VideosGVCNN: Group-View Convolutional Neural Networks for 3D Shape RecognitionAn End-to-End TextSpotter with Explicit Alignment and AttentionStereoscopic Neural Style TransferBootstrapping the Performance of Webly Supervised Semantic SegmentationLearning Markov Clustering Networks for Scene Text DetectionCollaborative and Adversarial Network for Unsupervised domain adaptationCollaborative and Adversarial Network for Unsupervised domain adaptationReflection Removal for Large-Scale 3D Point CloudsPose Transferrable Person Re-IdentificationLearning to Adapt Structured Output Space for Semantic SegmentationLearning to Adapt Structured Output Space for Semantic SegmentationEfficient Diverse Ensemble for Discriminative Co-TrackingLearning a Single Convolutional Super-Resolution Network for Multiple DegradationsProbabilistic Plant Modeling via Multi-View Image-to-Image TranslationLearning to Parse Wireframes in Images of Man-Made EnvironmentsA Variational U-Net for Conditional Appearance and Shape GenerationA Variational U-Net for Conditional Appearance and Shape GenerationLearning to Find Good CorrespondencesLearning to Find Good CorrespondencesActor and Action Video Segmentation from a SentenceActor and Action Video Segmentation from a SentenceTowards a Mathematical Understanding of the Difficulty in Learning with Feedforward Neural NetworksWeakly-supervised Deep Convolutional Neural Network Learning for Facial Action Unit Intensity EstimationMaximum Classifier Discrepancy for Unsupervised Domain AdaptationMaximum Classifier Discrepancy for Unsupervised Domain Adaptation由于微信字数限制，没有全部显示，详细 list 请查看 Amusi 整理的https://github.com/amusi/daily-paper-computer-vision\n",
      "'gbk' codec can't encode character '\\ufeff' in position 10: illegal multibyte sequence\n",
      "error:  \n",
      "error: 分词api请求失败多次！北联大专业知多少——第八辑：管理学院！！！。！！！北京联合大学北四环校区北京联合大学管理学院是一所以本科教育为主体，致力于培养经济管理类人才的学院！！！。！！！设有金融学、工商管理、会计学、财务管理、电子商务五个专业，在校生约2200 人！！！。！！！学院是北京联合大学国家级服务外包人才培养模式创新试验区核心单位，拥有国家级特色专业建设点金融学专业和市级经贸实验教学示范中心！！！。！！！同时学院招收“工商管理( 企业管理、会计学)”学术型硕士研究生，“金融”、“职业教育（财经商贸）”专业型硕士！！！。！！！金融学( 国家级特色专业建设点)本专业面向北京金融服务业，培育科技与人文素质兼备，理论与实务知识并重，具有扎实的经济学、金融学理论基础，通晓主要金融机构的业务流程，熟悉金融市场和产品，具备较强分析与解决问题的能力，具有国际视野、创新创业精神和社会责任感，可在政府各级管理部门、金融机构以及工商企业等部门从事金融投资分析、金融产品营销、客户服务和业务拓展等岗位工作的的复合型、应用型人才！！！。！！！毕业生既可在商业银行，证券公司和保险公司从事金融产品营销、金融投资分析和金融业务管理工作；也可在互联网金融公司、担保公司、小额贷款公司、信用级机构、工商企业等机构担任资本运营人员！！！。！！！工商管理 本专业培养能够服务地方经济发展需要，系统地掌握工商企业管理及国际商务管理的理论知识与技能，具备经济管理、法律、计算机应用等方面知识，熟悉通行的国际商务规则和惯例，有较好的外语和信息技术的应用能力，具有国际视野、基础扎实、社会责任感和实践能力较强、具有一定实务性研究和创新创业能力的中高级复合型、应用型经营管理人才！！！。！！！工商管理专业就业领域宽、毕业生适应面广，可以在各类工商企业从事人力资源、产品运营、市场策划与管理、行政管理等工作，也可以在政府机关及学校从事行政管理工作，还可以报考研究生或出国深造，以及自主创业！！！。！！！会计学 本专业培养适应经济建设和社会发展需要，系统掌握会计学基本理论、基本技能，掌握和具备管理、经济、法律等方面的专业知识和能力，基础扎实、实践能力强、工作作风实，具有创新创业精神、社会责任感和良好的职业道德，具有较强的适应能力和终生学习能力，能在企事业单位、会计师事务所及政府部门从事会计实务、审计实务、财务管理和相关专业工作的具有专业素质的应用型人才！！！。！！！对外合作办学：与美国IMA合作，将美国CMA 考试课程体系嵌入培养计划，每年组织学生参加CMA 项目培训考试！！！。！！！与爱尔兰阿斯隆理工学院2+2、3+1合作项目效果良好！！！。！！！参加项目的学生有数人进入世界百强校（爱尔兰圣三一学院）读研，毕业后进入国际四大等机构工作！！！。！！！财务管理 财务管理专业是一个非常有前途，也很容易规划发展路径的专业！！！。！！！现实生活中，对所有组织来说，人、财、市场都是最为关键的因素！！！。！！！特别是企业，财务规划、财务控制和投资、资本运作、融资、金融等等都有千丝万缕的联系，其职业发展可谓是前途无限！！！。！！！本专业注重课程创新，构建“学研练赛”的课程体系；名师领衔高水平师资，开拓校企融合的多方位视野；接轨国际人才培养模式，将CGMA课程嵌入教学体系；开展学科竞赛助推学生成长，通过社团活动全面锻炼能力；拓展高水平就业渠道，提供高质量求学通道！！！。！！！电子商务 电子商务专业培养通晓管理、经济、计算机和电子商务等知识，具有国际视野、创新创业素质和较强的社会责任感，具备互联网思维、实践能力和终身学习能力，掌握电子商务网站建设运营、网络营销策划与实施、数据分析与管理等方面的专业知识和技能，能够在电子商务和互联网企业、电子商务服务企业以及各类企事业单位的相关部门从事互联网环境下的商务运营、专业管理和技术服务等工作，成为电子商务领域的中高级复合型、应用型人才！！！。！！！本专业设置了商务运营与管理、商务数据分析两个专业方向！！！。！！！毕业生就职于百度、京东、腾讯、网易、新浪、苏宁等各大知名互联网企业！！！。！！！在各类电子商务服务企业和企事业单位的相关部门，从事互联网运营管理、网络营销、互联网产品策划、数据分析、项目管理、程序设计等工作，深受用人单位欢迎！！！。！！！编辑：陈思阳来源：北京联合大学高考招生办公室\n",
      "'gbk' codec can't encode character '\\u2022' in position 12: illegal multibyte sequence\n",
      "error:  峰会报名网址：http://www.huodongxing.com/event/7433112291300时间：2018年4月21日地址：越南河内大宇酒店联系我们：招商/票务/媒体合作：170-3113-5408 陈女士如需协助办理机票、酒店，可联系：159-0192-9442 许女士项目采访 首发|发币谁都会，你会发“货”么？是时候用Token干点“正经事”了电商“有病”，区块链“有药”吗？正逢数字货币乱世，他们想用技术来保护投资人的最基本权益数字货币交易所何去何从？用区块链技术实现影响力的商业价值用区块链技术拉起股权交易市场新的增长曲线基于智能合约和区块链技术的创新信贷交换平台专家专栏 既要懂技术又懂产业，2018将是区块链正规军入场元年硅谷资深投资人讲析区块链项目投资|教程王玮：区块链通证架构的思辨软件好，才是真的好：区块链的19762017信仰和投机：币圈没有奇迹与元道对话三：区块链经济正在进行“动力切换”百家观点 如何设计区块链项目的通证（token）模型加密货币和区块链（一）：历史的重演裸照与区块链社群疯狂的韩国比特币市场：“全民”炒币，人均收益率425%开年反攻：泡沫中的token和被冷落的联盟链对零识感兴趣的记者/活动/商务请甩Resume至：hr@zkchainnews.cn\n",
      "'gbk' codec can't encode character '\\u200b' in position 18: illegal multibyte sequence\n",
      "error:  几个月前我给一批币圈朋友讲课，讲到了各种协议和应用方法，涉及面广了一些，大家都听得很兴奋，然后问我在做什么\n",
      "error: 分词api请求失败多次！附：详细名单如下：（上下滑动查看完整名单）天 津 玖龙纸业（天津）有限公司天津广聚源纸业集团有限公司天津市宝坻区发达造纸有限公司河 北 河北华泰纸业有限公司石家庄东大纸业有限公司秦皇岛金茂源纸业有限公司秦皇岛市抚宁县丰满板纸有限公司秦皇岛沅泰纸业有限公司昌黎县兴昌纸业有限责任公司河北永新纸业有限公司河北昌泰纸业有限公司玉田县富兴纸业有限责任公司唐山市丰南区盛达纸业有限公司保定市满城成功造纸厂保定市满城金光纸业有限公司河北省保定市东方造纸有限公司保定市三联纸业有限公司沙河市锦新纸业有限公司保定市立发纸业有限公司石家庄大章纸业有限公司保定市满城红升纸业有限责任公司保定鼎泰纸业有限公司保定市中信纸业有限公司石家庄市广利纸业有限公司保定华康纸业有限公司满城县晨松造纸厂满城县汇丰纸业有限公司保定市满城利军造纸有限公司河北兴荣纸业有限公司沧州瑞雄纸业有限公司河北亚光纸业有限公司河北姬发造纸有限公司保定市豪峰造纸厂霸州市华通纸业有限公司保定市诚信纸业有限公司保定市满城县眺山营福利造纸厂保定市满城县福利造纸厂河北大发纸业有限公司抚宁县恒宇造纸厂石家庄市金东纸业有限公司玉田县大众宝来纸业有限公司保定市新宇纸业有限公司保定市顺通纸制品厂（普通合伙）抚宁县华云纸业有限公司保定市港兴纸业有限公司山 西 太原家盛纸业有限公司太原市晋源区吉兴造纸厂山西华南纸业股份有限公司忻州市瑞隆纸业有限公司山西强伟纸业有限公司山西省外贸平遥包装印刷（集团）造纸有限公司山西则天浆纸有限公司文水县天成纸业有限公司山西林纸造纸有限公司临县创新纸业有限公司山西省交城森海造纸有限公司山西运城市瑞马纸业有限公司运城市自强纸业有限公司临猗县力达纸业有限公司芮城县中宝纸业有限公司山西志峰农科贸有限公司山西鸿昌农科贸有限公司山西合盛农工贸有限公司稷山县联达造纸厂稷山县鹏腾工贸有限公司芮城县汇宝纸业有限公司内 蒙 内蒙古天浩纸业有限公司内蒙古大兴安岭浆纸有限责任公司辽 宁 辽宁金叶纸业有限公司玖龙纸业（沈阳）有限公司大连金洋纸业有限公司大连丰源纸业有限公司大连宝发纸业有限公司辽宁振兴生态造纸有限公司抚顺琥珀纸业有限责任公司辽宁铭笙纸业有限公司丹东洪阳纸业有限公司锦州金日纸业有限责任公司锦州金海纸业有限公司阜新天合纸业有限公司辽阳市顺航纸业有限公司辽宁兴东纸业有限公司辽宁豪唐纸业有限公司和合卫生用品有限公司福兴纸业有限公司广源纸业有限公司辽宁尚阳纸业有限公司柏吉纸业 吉 林吉林晨鸣纸业有限责任公司白山市虹桥纸业有限公司吉林晨鸣纸业有限公司吉林市振源纸业有限公司吉林市恒源纸业有限公司东丰县豪源纸业有限公司东丰县宏达纸包装材料有限公司吉林海川再生环保有限责任公司梅河口市海山纸业有限责任公司延边石岘双鹿实业有限责任公司黑龙江 牡丹江恒丰纸业集团有限责任公司牡丹江市大安纸板厂牡丹江市三都特种纸业有限公司海林市柴河林海纸业有限公司佳木斯东方纸业有限公司杜尔伯特县海达纸业有限公司大庆宏桥纸业有限责任公司密山市银河纸业有限公司鸡西市滴道区恒源纸制品厂伊春永丰纸业有限公司鹤岗市金山纸业有限责任公司黑龙江银泉纸业有限公司黑龙江泉林生态农业有限公司肇东东顺纸业有限公司哈尔滨铭源纸业有限公司哈尔滨大东方新材料科技股份有限公司大连运城制版有限公司哈尔滨分公司哈尔滨和鑫造纸有限公司哈尔滨亿阳纸业有限公司哈尔滨秉信纸业有限公司双城市金海岸纸业有限责任公司黑龙江省新承包装有限公司哈尔滨哈烟包装材料工业公司哈尔滨市中北包装材料厂江 苏 南京星光再生纸业有限公司江阴市富豪纸业有限公司江阴市金泰纸业有限公司江阴市天河纸业有限公司无锡市生力纸业有限公司江阴市勤丰纸业有限公司江阴市月城中新纸业有限公司江阴比图特种纸板有限公司理光感热技术（无锡）有限公司无锡市兴龙纸业有限公司江阴市兴港纸业有限公司江阴新浩再循环纸业有限公司无锡荣成环保科技有限公司无锡市东湖文化用品有限责任公司徐州市长兴纸业有限公司徐州金伦纸业有限公司徐州中兴纸业有限公司江苏尚品大成纸业科技有限公司江苏星光新材料科技有限公司溧阳康瑞德恒纸业有限公司常州市金坛兴辉纸业有限公司金坛丰瑞包装有限公司江苏金湟纸业有限公司常州天宏纸品有限公司祥恒（常州）包装有限公司江苏嵘隆纸制品科技有限公司常州市鼎晨包装有限公司常州旺宇纸业有限公司江苏博尔纸制品科技有限公司常州市惠信纸业有限公司永丰余家品（昆山）有限公司江苏理文造纸有限公司苏州泰盛环保纸业有限公司芬欧汇川（中国）有限公司金华盛纸业（苏州工业园区）有限公司国一制纸（张家港）有限公司金红叶纸业集团有限公司玖龙纸业（太仓）有限公司海安县金鑫纸业有限公司南通市富发纸业有限公司江苏王子制纸有限公司南通富泰纸业有限公司南通富强纸业有限公司连云港市青蓝纸业有限公司灌云县小伊造纸厂连云港赣榆强盛纸业有限公司连云港市华成纸业有限公司连云港市东浦纸业有限公司灌云县富源纸业有限公司江苏博汇纸业有限公司江苏天成纸业有限公司江苏亭湖经济开发区华泰纸业有限公司胜达集团江苏双灯纸业有限公司江苏富星纸业有限公司江苏京环隆亨纸业有限公司盐城金龙发纸业有限公司盐城市开源纸业有限公司江苏凤程纸业有限公司涟水永丰纸业有限公司淮安恒发纸业有限公司洪泽县达仁纸品厂江苏飞翔纸业有限公司淮安金岳纸业有限公司涟水协欣纸业有限公司江苏洪泽湖纸业有限公司淮安市飞翔高新包装材料有限公司洪泽金百德纸业有限公司江苏金莲纸业有限公司江苏国圣纸业有限公司江苏新丰纸业有限公司扬州国安纸业有限公司永丰余造纸（扬州）有限公司永丰余生活用纸（扬州）有限公司丹阳市埤城造纸厂金东纸业（江苏）股份有限公司镇江大东纸业有限公司江苏长丰纸业有限公司丹阳市天由纸业有限公司江苏双蝶集团有限公司沭阳博大包装材料有限公司江苏弘盛纸业有限公司江苏宁沭纸业有限公司沭阳俊达纸业有限公司宿迁腾信纸业有限公司江苏上善纸业有限公司江苏凯盛纸业有限公司江苏誉凯实业有限公司浙 江 杭州秉信纸业有限公司杭州胜铭纸业有限公司浙江大胜达包装股份有限公司杭州萧山南阳造纸有限公司杭州诚品实业有限公司浙江亚欣包装材料有限公司浙江乾鑫帆纸业有限公司浙江省临安市金洲纸业有限公司杭州大伟装饰材料有限公司杭州临安正达纸业有限公司杭州临安鸿盛纸业有限公司杭州临安桃源纸业有限公司杭州海鑫纸业有限公司浙江帝龙新材料有限公司杭州华旺新材料科技有限公司宁波思特雷斯金属防护材料有限公司宁波亚洲浆纸业有限公司宁波中华纸业有限公司宁波牡牛集团有限公司宁海县宁兴纸业有限公司宁波市东腾纸业有限公司慈溪市晨阳包装有限公司慈溪福山纸业橡塑限公司浙江华康纸业有限公司嘉兴市虹亚纱管纸业有限公司民丰特种纸股份有限公司浙江秀舟纸业有限公司嘉兴市高翔纸业有限公司浙江大华包装集团有限公司浙江中顺纸业有限公司嘉兴福鑫纸业有限公司森立纸业集团有限公司浙江正华纸业有限公司浙江弘安纸业有限公司嘉兴大洋纸业股份有限公司嘉兴市博莱特纸业有限公司海盐县华联纸业有限责任公司海盐县造纸厂吉安集团有限公司浙江荣晟环保纸业股份有限公司浙江荣成纸业有限公司浙江景兴纸业股份有限公司桐乡福利造纸厂浙江新越纸业有限公司德清县东港纸业有限公司德清县华康纸业有限公司德清县上峰纸业有限公司德清县方毅纸业有限公司德清县双桥纸业有限公司湖州新天纸业有限公司湖州市韶春纸业有限公司湖州康力纸业有限公司湖州立丰纸业有限公司浙江华丰纸业科技有限公司浙江唯尔福纸业有限公司浙江华宇纸业有限公司绍兴县天明纸业有限公司上峰集团有限公司嵊州市西鲍第五纸业有限公司嵊州市永利纸业有限公司嵊州市恒丰纸业有限公司嵊州市宇信纸业有限公司嵊州市天宇纸业有限公司嵊州市宇丰纸业有限公司嵊州市白云纸业有限公司金华金星纸业有限公司金华金岭造纸有限公司浙江金华丁丁实业有限公司浙江武义张氏包装实业有限公司浙江武义要巨纸业有限公司浙江兰塘纸业有限公司兰溪市华圣纸业有限公司浙江华川实业集团有限公司浙江莱勒克纸业有限公司浙江夏王纸业有限公司浙江鑫丰特种纸业有限公司衢州五洲特种纸业有限公司浙江晶鑫特种纸业有限公司仙鹤股份有限公司浙江爱丽莎环保科技股份有限公司浙江常林纸业有限公司浙江舜浦纸业有限公司浙江金昌纸业有限公司浙江圣丰纸业有限公司浙江华邦特种纸业有限公司浙江天耀纸业有限公司浙江凯丰新材料股份有限公司浙江恒达新材料股份有限公司龙游塔恩纸业有限公司维达纸业（浙江）有限公司浙江金龙纸业有限公司浙江凯宝华新材料有限公司浙江一树纸业有限公司浙江大盛新材料股份有限公司浙江杭星新材料有限公司华邦古楼新材料有限公司浙江佳维康特种纸有限公司浙江海景纸业有限公司浙江凯丰特种纸业有限公司浙江腾辉科技有限公司江山市华盛纸业制造有限公司阿尔诺维根斯（衢州）特种纸有限公司浙江五星纸业有限公司衢州驰星包装有限公司衢州双熊猫纸业有限公司台州市永丰纸业有限公司台州市黄岩华丰纸业有限公司台州市家得宝科技有限公司台州欣荣鞋材科技股份有限公司台州森林造纸有限公司温岭市新华纸业有限公司台州森林彩印包装有限公司浙江豪博鞋材有限公司浙江东泰纸业有限公司浙江众发实业有限公司浙江新都纸业有限公司浙江凯恩特种材料股份有限公司安 徽 安徽虹光企业投资集团有限公司安徽省萧县林平纸业有限公司宿州市永兴纸业有限公司安徽灵璧东风纸业有限公司安徽鑫光新材料科技股份有限公司安徽康鑫纸业有限公司怀远县三联纸业有限责任公司安徽开来纸业有限公司临泉县盛源纸业有限公司太和县鸿盛纸业有限公司安徽兆隆纸业有限公司安徽霍山县晨风纸业有限公司山鹰国际控股股份公司安徽比伦生活用纸有限公司恒安（芜湖）纸业有限公司绩溪县力达纸业有限公司安徽浙源再生纸业科技有限公司东至县东方纸业有限公司安徽华泰林浆纸有限公司安徽万邦高森造纸有限公司黄山金仕特种包装公司安徽华邦特种纸业公司福 建 福清市金林福利纸品厂福建省闽清双棱纸业有限公司福建省恒兴纸业有限公司长乐宝贵福利生态包装材料有限公司福州闽辉鞋材有限公司闽侯县和丰纸业有限责任公司福清市金林纸品有限公司长乐巨中辉纸业有限公司厦门市同安兴浪纸业有限公司德彦纸业(厦门)有限公司厦门市麒龙纸业有限公司福建省尤溪永丰茂纸业有限公司福建腾荣达制浆有限公司福建省建宁县联丰造纸有限公司福建铙山纸业集团有限公司福建省青山纸业股份有限公司福建省大田县弘惠纸业有限公司福建省尤溪县青盛纸业有限公司沙县华佳纸业有限公司福建省沙县德利纸业有限公司尤溪三联纸业有限公司福建省沙县盛春纸业有限公司福建华闽纸业有限公司永安市宝华林特种纸业有限公司福建恒利纸业有限公司福建恒利集团有限公司福建泉州群发包装纸品有限公司恒安（中国）纸业有限公司泉州贵格纸业有限公司泉州华祥纸业有限公司福建省全诺纸箱包装有限公司福建省晋江优兰发纸业有限公司玖龙纸业（泉州）有限公司泉州联新纸业有限公司福建省晋江市隆英再生造纸有限公司福建省南安市三龙纸业有限公司南靖县雄发纸业有限公司漳州联盛纸业有限公司南靖县鑫利达纸业有限公司漳州华荣纸业有限公司华发纸业(福建)股份有限公司漳州盈晟纸业有限公司敦信纸业有限责任公司漳州八龙纸业有限公司龙海市榜山民政三星造纸厂漳州友利达纸业发展有限公司福建希源纸业有限公司漳州港兴纸品有限公司福建省联盛纸业有限责任公司联盛纸业(龙海)有限公司凤竹(漳州)纸业有限公司南靖县恒华纸制品有限公司南靖县鑫福纸业有限公司南靖县林星纸业有限公司南靖县柒星纸业有限公司南靖县吉利纸业有限公司南靖益龙纸业有限公司福建华发包装有限公司漳州市银湖纸制品有限公司福建中联纸业有限公司福建省建瓯市中亿纸品制造有限公司福建省建瓯市太平洋纸业有限公司福建利树浆纸有限公司福建利树股份有限公司福建省松溪县三维利纸业有限公司福建省莲龙纸业股份有限公司连城县金龙纸业有限公司连城县东方经济开发有限公司福鼎市南阳纸业有限公司福建省祥安纸业有限公司江 西 南昌市民升制品有限公司南昌旺鑫纸业有限公司江西晨鸣纸业有限责任公司赣州华劲纸业有限公司江西理文造纸有限公司江西华南纸业有限公司宜黄县星泰纸业有限公司江西金安包装新材料有限公司江西富临纸业有限公司抚州市富圣实业有限公司抚州银丰纸业有限公司抚州市金峰包装新材料有限公司抚州利峰纸业有限公司南城县永泰纸业有限公司江西鑫福源纸业有限公司抚州天新环保纸业有限公司江西顺达纸业有限公司江西省顺丰纸业股份有限公司江西三禾纸业有限公司广丰县芦林纸业有限公司江西裕丰纸业有限公司广丰县华龙实业有限公司江西奇丽印刷有限公司南昌方正包装有限公司南昌爱辉纸业有限公司江西特种纸业有限公司江西客家彩印包装有限公司赣州市五洲纸业有限公司信丰荣兴纸业有限公司江西吉富环保板有限公司全南县高家庄纸品塑料厂兴国县华伦纸业有限公司兴国佳新工贸有限公司瑞金市晶山纸业有限公司赣州华利纸品有限公司江西恒森纸业有限公司江西共青东升包装有限公司江西共青统百利包装有限公司瑞昌市瑞翔再生纸业有限责任公司江西绮玉纸业有限公司江西欣祥包装彩印有限公司江西弘泰纸业有限公司江西金阳砂纸有限公司江西南城金恒纸业有限公司广昌县昌圣再生纸业有限公司广丰县双鼎纸业有限公司余干县双达纸业有限公司江西明盛实业有限公司吉安丰顺纸业有限公司峡江县大华纸业有限公司峡江县富兴纸业有限公司江西富丰实业有限公司江西恒信包装有限责任公司泰和县金丰实业有限公司泰和县华胜实业有限公司江西登泰实业有限公司江西立峰纸业有限公司江西南方纸业有限公司宜春市金太阳制品厂宜春市秀江纸业有限公司江西梁氏纸业有限公司江西三森纸业有限公司奉新县季布诺纸业有限公司江西富宏纸业有限公司奉新宋埠造纸厂江西双林纸业有限公司成志（江西）包装有限公司宜丰县金亿纸业有限公司江西协升纸业有限公司宜丰县明晟纸业有限公司宜丰县南方实业有限公司宜丰县天马造纸厂铜鼓县长红造纸厂万载县田江造纸厂万载县青林造纸厂江西省万载县万盛纸业有限公司万载县康强纸业有限公司莲花县纸业有限公司萍乡市旭日纸业有限公司萍乡众大高新材料有限公司上栗县春南再生纸厂上栗县恒达纸业有限公司上栗县萍锋纸业有限公司萍乡市安源区永胜废纸再生加工厂萍乡市振兴造纸有限责任公公司山 东 山东天阳纸业有限公司济南灏源纸业有限公司永丰余纸业（青岛）有限公司青岛胶南瑞源纸业有限公司青岛恒广泰包装有限公司平度市长乐福利纸制品厂青岛海王纸业股份有限公司山东仁丰特种材料有限公司山东标典纸业有限公司山东博汇集团有限公司山东贵和纸业集团有限公司山东辰龙纸业股份有限公司齐峰新材料股份有限公司淄博华鹏纸业有限公司淄博新华纸业有限公司枣庄华润纸业有限公司远通纸业（山东）有限公司枣庄市海象纸业有限公司山东丰源中科生态科技有限公司山东博闻纸业有限责任公司滕州市华闻纸业有限公司枣庄市恒宇纸业有限公司滕州市民政造纸厂山东建丰纸业有限公司山东秦世集团有限公司山东荣华纸业有限公司东营华泰清河实业有限公司华泰集团有限公司山东斯道拉恩索华泰纸业有限公司莱州鲁通特种纸业有限公司龙口玉龙纸业有限公司烟台隆祥纸业有限公司烟台市大展纸业有限公司莱阳银通纸业有限公司山东晨鸣纸业集团股份有限公司山东万豪纸业集团股份有限公司山东恒安纸业有限公司潍坊恒联浆纸有限公司潍坊恒联新材料股份有限公司潍坊恒联美林生活用纸有限公司潍坊恒联特种纸有限公司汇胜集团股份有限公司山东世纪阳光纸业集团有限公司潍坊友谊纸业股份有限公司潍坊华港包装材料有限公司山东太阳控股集团有限公司山东联合纸业有限公司山东太阳宏河纸业有限公司济宁金升纸业有限公司肥城东升纸业有限公司山东天和纸业有限公司东顺集团股份有限公司泰安中泰纸业有限公司山东君上纸业有限公司山东德广工贸有限公司山东省东平县华东纸业有限责任公司东平县兴州纸业有限责任公司泰安百川纸业有限责任公司山东凯丽特种纸股份有限公司日照华泰纸业有限公司山东亚太森博桨纸有限公司山东百伦纸业有限公司维达纸业（山东）有限公司山东圣雅洁纸制品有限公司山东新凯电子材料有限公司临沂利华纸业有限公司山东光华纸业集团有限公司山东庞疃纸业有限公司山东永泰纸业有限公司沂水华淦纸品有限公司沂水鑫源纸业有限公司费县宏基纸业有限公司平邑县金源包装材料厂华星纸业有限公司平邑金太阳纸业有限公司山东威凯瑞纸业有限公司临沂玉龙工贸有限公司德州华北纸业有限公司德州泰鼎新材料科技有限公司德州方源纸业有限公司山东德派克纸业有限公司汇胜集团平原纸业有限公司山东江河纸业有限责任公司山东冠军纸业有限公司山东泉林纸业夏津有限公司禹城市北辰新型材料有限公司禹城市盛裕纸制品厂宁津县宝典纸业有限公司中冶银河纸业有限公司山东金蔡伦纸业有限公司山东省博兴县万松纸业有限公司邹平汇泽实业有限公司山东省博兴县华辰纸业有限公司山东省博兴县欧华特种纸业有限公司山东省博兴县兴华纸业有限公司菏泽鲁西南纸业有限公司菏泽牡丹纸业有限公司菏泽宏泰纸业有限公司山东天元集团有限公司巨野县天丰纸业有限公司山东泉润纸业有限公司单县华康纸业有限公司单县新华造纸有限公司菏泽喜群纸业有限公司菏泽鲁晨实业有限公司巨野鲁丰纸业有限公司河 南 新乡新亚集团股份有限公司河南兴泰纸业有限公司河南江河纸业股份有限公司濮阳龙丰纸业有限公司河南仙鹤特种浆纸有限公司驻马店市白云纸业有限公司邓州市华鑫纸业有限公司湖 北 武汉金凤凰纸业有限公司武汉晨鸣汉阳纸业股份有限公司武汉木兰汉北集体有限公司宜昌书林纸业有限公司（宜昌）湖北宜昌翔陵纸制品有限公司（宜昌）湖北宝塔沛博循环科技有限公司（宜昌）湖北鑫物再生纸业有限公司（宜昌）湖北金庄科技再生资源有限公司（宜昌）宜都清江纸业有限公司秭归县恒丰纸业有限公司湖北舒云纸业有限公司（宜昌）湖北华海纤维材料科技股份有限公司（襄阳南漳）湖北广发纸业有限公司（宜城）湖北拍马纸业有限公司湖北骏马纸业有限公司监利大枫纸业有限公司湖北荣成再生科技有限公司（松滋）公安县真诚造纸有限责任公司湖北秦楚纸业有限公司（公安）湖北祥兴纸业科技有限公司（监利）维达纸业（孝感）有限公司中顺洁柔（湖北）纸业有限公司恒安集团（湖北）有限公司金红叶纸业（湖北）有限公司金凤凰纸业（孝感）有限公司湖北森源纸业有限公司（孝感孝南）钟祥市应强纸业有限公司湖北雅都恒兴纸业有限公司（广水）湖北盛大纸业有限公司（仙桃）通城县红星纸业有限公司赤壁晨力纸业有限公司湖 南 湖南瀚星纸业有限公司湖南壹帆纸业有限责任公司浏阳市永泰纸业有限公司洞口丰晟纸厂湖南冰纯式纸业有限公司衡山县宇科纸业有限公司岳阳华丰纸业有限公司湖南华耀浆纸有限公司双峰县上峰造纸厂醴陵市王仙镇福利纸箱厂湘乡环松纸业有限公司汨罗市三江福利造纸厂汨罗市宏达教育造纸厂湖南汨江纸业有限公司宜章县创优纸业有限公司宜章林茂纸业有限公司汨罗市八里纸业有限公司湖南井田人纸业有限公司平江县湘南纸箱包装有限公司衡阳福麟纸业股份有限公司茶陵县中宇纸业有限公司邵东龙达纸业有限公司郴州强丰纸业有限公司浏阳市锦鸿纸业有限公司娄底市中伟纸品包装有限公司长沙金红叶纸业有限公司洪江市福利纸业有限责任公司浏阳市银河纸业有限公司浏阳市杨家纸业有限责任公司浏阳市银湖纸业有限公司醴陵市浦口天符造纸厂汨罗市三兴再生纸业有限公司益阳市大通湖恒顺纸业有限公司湖南安妮特种涂布纸有限公司郴州市裕农纸业有限公司洪江市海得利纸业有限公司澧县宏达纸管制造有限公司桃江县振兴再生纸业有限公司桃江县古杉再生纸厂洞口县林浪包装纸厂浏阳市富达纸业有限公司桃江县五福纸业有限责任公司新邵县鸿发纸业有限公司新邵德信绝缘纸板有限公司岳阳市正仁再生纸业科技开发有限公司汨罗市湘宏造纸厂醴陵市军山造纸厂汨罗市红星造纸厂湘潭市腾飞再生纸业有限公司湖南长沙双桥纸业化工有限公司沅江市恒达纸业有限责任公司湘潭湘华纸厂汨罗市天宝造纸有限责任公司绥宁县吉升工业纸板有限责任公司临澧县金渲纸业有限公司浏阳市杨花福利造纸厂邵东县廉桥造纸有限责任公司邵阳市广兴造纸有限责任公司浏阳市宏源造纸厂新邵县金龙纸业有限责任公司邵阳市诚信造纸有限公司郴州新骏纸品包装有限公司南县建华纸业有限责任公司浏阳市文家市星华纸厂湖南省宁乡县乐邦纸业有限责任公司醴陵市永胜纸厂浏阳市连心造纸厂浏阳市南竹山纸业有限公司湖南湘一纸业有限公司宁远展源造纸有限公司醴陵市达文纸厂常德市鼎城文福纸业有限公司浏阳市明仁造纸厂浏阳市红新纸业有限公司常德市鼎城洞庭纸业有限公司浏阳市官渡造纸厂浏阳市晨鸣纸业有限公司安乡桂华纸业有限公司邵阳市先锋纸业有限公司浏阳市恒隆造纸厂洞口县靓洁拷贝纸有限责任公司沅江市森裕顺纸业有限公司城步苗族自治县银河纸业有限责任公司汨罗市恒美纸业有限公司浏阳市鑫源造纸厂湖南飞页纸业有限公司浏阳市顺泰纸业有限责任公司会同县宝庆恒达纸业有限公司临澧县伟临纸制品有限公司汨罗市长盛纸业有限公司湖南双华纸业有限公司新邵县大花纸制品有限公司岳阳县富华纸业有限公司邵阳市长兴造纸厂桃江县金沙纸业有限公司岳阳县洞庭纸厂新邵县富源造纸厂衡阳市鸿康纸塑容器包装有限公司临湘市兴达再生纸厂浏阳市振兴纸业有限公司临澧宏鑫科技发展有限公司临澧县金利纸业有限责任公司常德市天耀纸业有限公司湖南恒安纸业有限公司湖南恒安生活用纸有限公司恒安（湖南）心相印有限公司湖南省鸿城纸业有限公司湖南五强溪特种纸业有限公司会同县春辉造纸有限公司会同县惠杰特种纸纸业有限责任公司新晃县自强纸业有限责任公司芷江恒兴纸制品厂芷江荣森生态纸业纸制品厂通道神华林化有限公司溆浦县中林造纸厂中方县隆盛纸业有限公司浏阳市天和纸业有限公司株洲市龙兴纸业有限公司永兴县正邦纸业有限公司永兴县和祥纸业有限公司永兴县兴旺纸业有限公司桂阳县宝顺再生纸业有限公司宜章县成平纸业有限公司宜章县四旺纸厂宜章县安达纸业有限公司宜章县新辉纸厂宜章吉兴纸业有限公司宜章县范家联合纸厂宜章光明包装制品有限公司永顺县洪飞纸厂永顺县通达纸厂永顺县鸿升纸业有限责任公司吉首市鸿达纸业有限公司湖南湘丰特种纸业有限公司隆回县祁都纸业有限公司新邵大源纸业有限公司邵阳市鸿源纸业有限公司邵阳市双清华恒包装纸厂嘉禾县兴旺纸业有限公司龙山县兴隆造纸厂邵东县黑石铺黄桥造纸厂邵东县黑石铺齐全造纸厂邵东县两市塘康桥造纸有限公司邵阳市双清区长丰造纸厂衡山新金龙纸业有限公司益阳市华发纸业包装有限公司沅江三眼塘恒发纸业有限公司南县森裕纸业有限公司邵阳市昆鹏造纸有限责任公司岳阳市蓝海造纸科技有限公司浏阳市大瑶兴宇造纸厂浏阳市太平桥金源造纸厂浏阳市锦辉纸业有限公司湖南东顺纸业有限公司浏阳市中旺纸业有限公司岳阳林纸股份有限公司岳阳分公司岳阳丰利纸业有限公司岳阳林纸股份有限公司沅江分公司沅江金太阳纸业有限公司湖南林源纸业有限公司绥宁县宝庆联纸有限公司绥宁县天成造纸有限公司广 东 广州市浩兴纸业有限公司广州市南沙区南沙华实纸品厂广州市花都长兴纸业有限公司广州市启鸣纸业有限公司广州市宏杰达纸业有限公司广州市聚益纸业有限公司广州万利达纸制品有限公司广州造纸股份有限公司增城永耀纸制品有限公司珠海华丰纸业有限公司珠海经济特区红塔仁恒纸业有限公司广东华粤安环保科技股份有限公司汕头市潮阳区金浦三保纸业加工厂汕头市澄海区溪南诚隆纸品厂汕头市潮阳区东六纸品有限公司汕头市潮阳区金浦伟佳纸品厂汕头市潮阳区恒欣纸品厂汕头市潮阳区金浦成兴纸品厂汕头市高盛纸业有限公司汕头市金平区飘合纸业有限公司广东万安纸业有限公司广东翊德环保纸业有限公司汕头市澄海区溪南乐华造纸厂汕头市澄海区溪南东社造纸厂汕头市龙湖区鑫隆纸类制品厂汕头市广利造纸有限公司汕头市广润纸品包装有限公司汕头市潮阳区金浦兴泰纸品厂广东松炀再生资源股份有限公司汕头市潮阳区金浦金辉盛纸品厂汕头市潮阳区金浦金创盛纸品厂汕头市潮阳区辉星纸品实业有限公司佛山市南海蓝天鹅造纸有限公司佛山市顺德区联信纸业有限公司佛山市金利澳纸业有限公司佛山市高明鸿源纸业有限公司佛山金盛联合纸业有限公司翁源县萍胜纸厂韶能集团韶关南雄珠玑纸业有限公司广东博泰纸业有限公司韶关市始兴县联兴造纸实业有限公司乐昌市永业纸制品有限公司蕉岭县双福建材有限公司徐溪造纸厂兴宁市宁新华贵造纸厂蕉岭金发纸业有限公司博罗县石湾镇华丰卫生纸厂惠州福和纸业有限公司博罗县石湾镇金辉卫生纸厂博罗县和兴纸业有限公司惠州泰美纸业有限公司博罗县惠盛纸业有限公司龙门县美林纸业有限公司博罗县凤达纸业有限公司海丰县吉丰纸业有限公司东莞玖龙纸业有限公司广东理文造纸有限公司东莞建晖纸业有限公司东莞金洲纸业有限公司东莞市金田纸业有限公司东莞顺裕纸业有限公司东莞理文造纸厂有限公司东莞市泰昌纸业有限公司东莞市骏业纸业有限公司东莞市潢涌银洲纸业有限公司东莞市双洲纸业有限公司东莞市鸿业造纸有限公司东莞市道兴隆造纸厂有限公司东莞市旭丰纸业有限公司东莞市上隆纸业有限公司东莞市中联纸业有限公司东莞市白天鹅纸业有限公司东莞市建航纸业有限公司东莞市新富发纸业有限公司东莞市建桦纸业股份有限公司广东比伦生活用纸有限公司东莞市祥兴纸业有限公司东莞市达林纸业有限公司东莞市汇隆特种纸业有限公司东莞市时和利造纸有限公司东莞市中桥纸业有限公司中山联合鸿兴造纸有限公司中山永发纸业有限公司中烟摩迪（江门）纸业有限公司开平市易大丰纸业有限公司江门市丰达纸业有限公司江门市明星纸业有限公司江门市洋汇纸业有限公司江门市桥裕纸业有限公司亚太森博（广东）纸业有限公司江门市长兴纸业有限公司江门星辉造纸有限公司开平镇威纸业有限公司广东阿博特数码纸业有限公司广东华泰纸业有限公司江门市新会区银湖纸业有限公司维达纸业（中国）有限公司广东分公司江门中顺纸业有限公司维达纸业（中国）有限公司维达纸业（中国）有限公司江门分公司阳春市嘉和纸业有限公司阳春市豪达纸业有限公司阳春市恒晖纸业有限公司阳春市华俊纸业有限公司广东省遂溪县嘉畅造纸厂吴川市山江纸厂廉江市广林纸业有限公司湛江华信纸业有限公司湛江晨鸣浆纸有限公司湛江市吉城纸业有限公司湛江冠豪纸业有限公司广东冠豪高新技术股份有限公司高州市文发纸业有限公司信宜市恒新包装材料有限公司高州市金墩纸业有限公司广东鼎丰纸业有限公司肇庆市中盛纸业有限公司广东珠江特种纸股份有限公司广宁县广安纸业有限公司肇庆科伦纸业有限公司封开县嘉诚纸业有限公司肇庆市中盛纸业有限公司石涧分公司森叶（清新）纸业有限公司潮州市潮安区树基纸品厂潮州潮安区弘和纸品有限公司揭阳市揭东区勤裕纸业有限公司揭阳市揭东联发造纸厂揭阳空港经济区炮台镇兴业造纸厂广东信达纸业有限公司中顺洁柔（云浮）纸业有限公司广东通力定造股份有限公司惠来县葵山造纸有限公司揭阳市铭湖纸业有限公司揭阳市龙新实业有限公司潮州市协成纸品有限公司潮州市潮安区凤塘东之阳纸板厂潮州市湘桥区辉业纸类制品厂潮州市潮安区凤塘镇大地包装材料厂潮州市潮安区骏发纸品有限公司潮州市潮安区凤塘玉窖纸类工艺制品厂连州市联发造纸有限公司台山市翔隆纸业有限公司江门市新会区崖门瑞兴造纸厂江门市新会区华悦纸厂江门市新会区宝达造纸实业有限公司江门仁科绿洲纸业有限公司江门市新会区七堡潭江造纸厂江门市新会区新飞纸厂江门市本创纸业有限公司江门市新会区银海纸业有限公司恩平三力胶粘制品有限公司阳春市联兴造纸厂阳春市三元造纸厂阳春市维美纸业有限公司阳春市长兴纸业有限公司湛江鹏宇纸业有限公司高州市富城造纸有限公司茂名市茂南昌和纸业有限公司广宁县顺盈造纸厂兴宁市龙丰造纸厂韶关市联进纸业有限公司汕头市澄海区莲上造纸有限公司汕头市澄海区诚信造纸厂汕头市澄海区广达造纸有限公司汕头市澄海区平安造纸厂汕头市澄海区洋新纸业有限公司汕头市澄海区源诚造纸厂汕头市澄海区振业纸品厂广 西 南宁君盈纸业有限公司南宁祈顺纸业有限公司南宁衍庆纸浆有限公司广西永凯大桥纸业有限责任公司广西永凯糖纸有限责任公司广西集盛纸品有限公司广西南宁东纸业有限公司南宁美纳纸业有限公司广西天力丰生态材料有限公司广西横县华宇工贸有限公司广西横县六景北墨造纸厂广西欣瑞纸业有限公司横县东糖糖业有限公司纸业分公司广西横县江南纸业有限公司广西浩林纸业有限公司南宁市上峰纸业有限公司南宁市圣大纸业有限公司马山和发强纸业有限公司南宁市佳达纸业有限责任公司广西凤糖鹿寨纸业有限公司柳州两面针纸业有限公司柳州市柳林纸业有限公司柳州中迪纸业有限公司柳州市丰源纸业有限责任公司柳州市鹿寨佳利造纸厂柳州市桂中纸业有限公司广西林业荔浦纸业有限公司桂林奇峰纸业有限公司桂林市艺宇印刷包装有限公司梧州市盈信造纸有限公司蒙山县大明纸业有限公司广西梧州市灵鹤造纸实业有限公司斯道拉恩索（广西）浆纸有限公司广西桂海金浦纸业有限公司防城港宏源浆纸有限公司广西金桂浆纸业有限公司广西贵糖（集团）股份有限公司广西贵港红旗纸业有限公司广西贵港市兴新纸业有限责任公司广西华怡纸业有限公司广西洁宝纸业投资股份有限公司广西贵港市华欣纸业有限公司贵港市安和纸业有限公司广西容县前景纸业有限公司广西容县红光纸厂陆川县裕源纸业有限公司广西容县新天地造纸厂广西劲达兴纸业有限公司田东县金荣纸业有限公司广西田阳华美纸业有限公司田阳南华纸业有限公司广西田东南华纸业有限公司百色市合众纸业有限责任公司田林县荔森纸业有限责任公司广西贺达纸业有限责任公司广西贺州市红星纸业有限公司广西博冠环保制品有限公司广西来宾东糖纸业有限公司广西永鑫华糖集团来宾纸业有限公司广西来宾华美纸业有限公司广西农垦集团天成纸业有限公司广西农垦集团华成纸业有限公司广西金竹源纸业有限公司来宾市华欣纸业有限公司合山市海盛纸业有限公司广西象州龙腾纸业有限责任公司广西象州鑫龙纸业有限公司龙州南华纸业有限公司广西龙州曙辉纸业有限公司广西崇左市大明纸业有限公司重 庆 武隆县灵烽纸业有限公司维尔美纸业（重庆）有限公司重庆市潼南简氏纸业包装有限责任公司重庆市南川区金鑫纸业有限公司重庆市江津区金川纸业有限公司重庆理文造纸有限公司玖龙纸业（重庆）有限公司重庆博大农牧产品进出口有限公司重庆市恒丰纸业有限公司梁平县邵新纸业有限公司梁平县渝丰纸制品有限公司重庆市铜梁区铜鑫纸业有限责任公司重庆市铜梁区钢隆纸业有限公司四 川 四川迅源纸业有限公司四川省宜宾市强盛纸业有限公司什邡市仁亮纸制品厂四川省汇禾泰纸制品有限公司宜宾蓝天纸业股份有限公司成都市海龙纸业有限公司成都联达纸业有限责任公司广汉市朝富纸业有限公司四川省汉源缘通有限责任公司威远县恒丰纸业有限公司四川华侨凤凰纸业有限公司四川内江金子山纸业有限公司成都森隆纸业有限公司眉山市阳光纸业有限公司玖龙纸业（乐山）有限公司什邡市今日纸业有限公司隆昌川东龙纸业有限公司乐山市五通桥区永华纸业有限公司四川金田纸业有限公司四川泰昌纸业有限公司成都美岭纸业有限公司四川省西昌市西溪（集团）股份公司纸板厂四川什邡市华利纸业有限公司绵竹仟森纸业有限责任公司丹棱万平纸业有限公司盐边县宏源纸业有限公司什邡市陈氏纸业有限公司四川新津晨龙纸业有限公司彭州市蒙阳奎江纸品有限公司北川羌族自治县葵林纸厂乐山市五通桥恒源纸业再生利用有限公司眉山市鸿宇纸业有限公司四川省广汉富华纸业有限公司成都市蒲江县五星顺江纸厂彭州市顺元纸业有限公司都江堰市黎明纸制品有限公司成都市岭源纸业有限公司大英县金龙纸业有限公司广汉兴华原纸业有限责任公司四川省中江县盛源纸业有限公司绵竹市鑫利来纸业有限公司富顺县安溪纸业有限公司汉源县鑫龙再生纸厂夹江鸿泰造纸厂达州市金月升纸业有限公司四川水都纸业有限公司遂宁市明都纸业有限公司邛崃市太平纸业有限公司大邑县悦来造纸二厂绵阳凯特包装材料有限公司四川省邛崃市众兴纸品厂广元市利州区平溪纸品加工厂遂宁市远明纸业有限公司乐山市盛世纸业有限责任公司乐山市东岳纸业有限责任公司叙永县三联纸业有限公司三分厂叙永县三联纸业有限公司一分厂叙永县三联纸业有限公司叙永县恒生纸业有限公司孙家河再生造纸厂叙永县恒生纸业有限公司美金再生造纸厂恒安（四川）生活用品有限公司四川省崇州市上元纸业有限公司自贡市生活用纸厂四川省芦山县兴业造纸厂-纸业公司犍为三环纸业有限公司泸州市圣峰纸业有限公司四川省三台三角生活用纸制造有限公司中顺洁柔（四川）纸业有限公司四川省万安纸业有限责任公司广安市安琪日用品有限公司四川省津诚纸业有限公司四川省绵阳市超兰卫生品有限公司内江市甜郁纸业有限公司崇州市黑石纸业有限公司自贡市东升纸业有限公司沐川禾丰纸业有限责任公司四川圆周实业有限公司彭州市五一纸业有限责任公司彭州市大良纸厂四川友邦企业有限公司成都居家生活造纸有限责任公司崇州市倪氏纸业有限公司成都志豪纸业有限责任公司成都绿洲纸业有限责任公司维达纸业（四川）有限公司四川腾龙纸业有限责任公司成都鑫宏纸品厂四川省德昌县环洁有限责任公司德昌县小高造纸厂洪雅县星星纸业有限公司四川省洪雅县金釜纸厂崇州市上元小中纸制品加工厂成都市川津纸业有限公司四川中达纸业有限公司四川蜀邦实业有限责任公司成都精华纸业有限公司成都市阿尔纸业有限责任公司彭州市欣美夫造纸厂彭州市红旗纸厂遂宁金红叶纸业有限公司眉山贝艾佳纸业有限责任公司乐山新达佳纸业有限公司四川省崇州市大方纸业有限责任公司彭州万兴纸厂彭州市红星纸厂成都百丽纸业有限责任公司资中县亚泰纸品有限公司资阳市纸制品厂（普通合伙）青白江区祥龙纸厂成都市青白江区香山纸厂青白江汇泉造纸厂成都市青白江区蓝明纸厂青白江区大同镇富惠纸制品厂成都市青白江区日新东方纸厂四川国木再生资源利用有限公司四川新惠阳农业特种纸业有限公司四川绿果林农业特种纸业有限公司成都建丰装饰纸有限公司成都帝豪新材料有限公司成都市青白江区升达环保装饰材料有限公司四川桦欣科技有限公司眉山瑞丰纸业有限公司乐山新宏业科技有限公司芦山县荣源纸品有限责任公司平昌县再生纸业有限责任公司眉山市新华纸业印务有限公司中江县清河乡造纸厂眉山市宏大纸业有限公司眉山市东坡区樱中再生纸有限公司四川眉山市丰泰纸业有限公司简阳市永和纸业有限公司泸州巨源纸业有限公司四川省眉山市五江纸业有限公司五龙造纸厂眉山市东坡区陈氏纸业工贸有限公司眉山石佛纸业有限公司夹江县金鑫商贸有限公司欣意纸厂泸州市洪都再生资源开发有限公司大英县献国纸业有限责任公司成都市金陈造纸有限公司岳池县银川再生纸厂富顺县沱江纸厂四川省眉山市五江纸业有限公司五龙造纸三分厂眉山市东坡区鲜滩纸业工贸有限公司眉山海银工贸有限公司大邑县久盛造纸厂四川省大英县三鑫纸业有限公司德昌县乐跃东升纸厂宜宾市南溪区恒达纸业有限公司四川永丰浆纸股份有限公司四川永丰纸业股份有限公司宜宾纸业股份有限公司四川省犍为凤生纸业有限责任公司四川银鸽竹浆纸业有限公司四川天竹竹资源开发有限公司四川省金福纸品有限责任公司夹江汇丰纸业有限公司四川省眉山丰华纸业有限公司四川省西龙生物质材料科技有限公司安县纸业有限公司四川省高县华盛纸业有限公司贵 州 安顺汇景卫生材料科技有限公司贵州昆泰纸业有限公司贵州富泰凯纸业有限公司赤水市天竹纸业有限公司贵州东阳纸业有限公司贵州赤天化纸业股份有限公司贵州裕纸业实业有限公司惠水县佳宇造纸厂贵州万军包装产业园有限公司贵州合成新材料有限公司贵州倍特纸业有限公司贵州普盛纸业有限公司贵阳黄果树纸业有限公司贵州方鸿包装有限责任公司贵阳金康纸业有限公司修文县宏源纸制品包装有限公司贵州万隆印务有限公司贵州年丰纸品有限公司遵义市绥阳县新长征纸箱包装有限公司遵义鸿达纸箱制品有限公司遵义县苟江镇鑫欣源纸业有限责任公司贵州省仁怀市富煌工贸有限责任公司贵州永盛纸业有限公司贵州省平坝果树园艺场纸箱厂贵州外贸神采包装有限责任公司贵州金鑫纸业有限公司贵州省安顺市德力包装发展有限公司贵州省顺鸿卫生用品有限公司贵州鹏（集团）纸业有限责任公司贵阳雨娇纸制品厂贵州自由纸业有限公司赤水市百冠纸品有限责任公司赤水市恒鑫科技发展有限公司安顺市永玖贸易有限公司贵州永祥纸业有限公司贵州省远航纸业制造股份有限公司贵州省澎萱纸业有限公司思南县桂春纸业有限责任公司印江自治县飞龙纸业有限公司贵州东明纸业有限公司贵州卡布国际卫生用品有限公司贵州省独山县华兴纸业有限公司独山利发纸业有限公司云 南 嵩明鹏森纸业有限公司云南李好纸业有限公司宜良县红星兄弟纸业有限公司宜良明丰造纸厂宜良创宇纸业有限公司云南东晟纸业有限责任公司富民宝地纸业有限公司昆明市宜良造纸厂昆明忠祥纸业有限公司宜良县凤鸣办事处双抚纸箱厂云南陆良银河纸业有限公司华宁县裕丰纸业有限公司云南省江川县恒昌造纸有限责任公司云南省通海县三义造纸厂云南新平南恩糖纸有限责任公司云南汉光纸业有限公司易门顺兴纸业有限公司保山鑫盛泰纸业有限公司绥江县中兴纸业有限责任公司云南云景林纸股份有限公司临沧南华纸业有限公司禄丰县永兴纸业有限公司弥勒康达彩印包装有限公司云南红塔蓝鹰纸业有限公司建水春秋纸业有限公司文山云荷纸业有限责任公巍山县明兴纸业有限责任公司大理华成纸业有限责任公司云南江川翠峰纸业有限公司玉溪市研和纸板制品厂云南明磊实业有限公司漾濞县宝隆纸业有限责任公司曲靖乾坤纸制品有限公司江川县丰茂纸业有限公司开远市衡越工贸有限公司云南玉溪水松纸厂云南通海云龙纸制品包装有限责任公司玉溪华宁昊兴纸业有限公司玉溪思润印刷有限公司昆明福瑞包装有限公司云南金晨纸业有限公司西 藏 西藏远征纸业股份有限公司拉萨经济技术开发区泰孚包装工贸有限公司西藏远征包装有限公司西藏坎巴嘎布卫生用品有限公司陕 西 西安渭丰纸业有限公司宝鸡科达特种纸业有限责任公司岐山县全兴纸业包装有限公司陕西圣龙纸业有限公司陕西法门寺纸业有限公司陕西兴翔纸业有限责任公司东方纸业集团有限公司欣雅纸业有限公司陕西大荔安盛纸业有限责任公司蒲城县残联东兴造纸厂大荔县高明纸业有限公司蒲城县永丰利亚造纸有限责任公司安康市恒丰纸业包装有限公司西安惠宁纸业有限公司甘 肃 静宁县恒达有限责任公司原料分公司平凉市宝马纸业有限责任公司平凉市峡门造纸厂临洮县红旗包装材料有限公司甘肃古浪惠思洁纸业有限公司天水轩辕纸业有限公司天水东方纸业有限公司兰州红安纸业有限公司张掖市光宇纸业有限公司张掖市明阳纸业有限责任公司甘肃江陇包装材料制造有限公司临泽金冠棉浆粕有限责任公司甘肃省瑞龙纸业有限责任公司宁 夏 宁夏紫荆花纸业有限公司中冶美利云产业投资股份有限公司新 疆 昌吉市江北再生纸业有限公司新疆恒发纸业有限公司玛纳斯源源纸业有限公司新疆远大纸业有限责任公司乌鲁木齐五星利强纸制品有限公司新疆希伯莱纸业有限责任公司玛纳斯源一实业有限公司新疆东盛纸业有限责任公司新疆华丽包装有限公司阿克苏盛祥彩印包装有限公司新疆元泰新材料有限公司新疆新美纸业有限公司乌鲁木齐秉信纸业有限公司温宿县弘泰农产品开发有限公司呼图壁县意达纸塑包装厂阿克苏市天星彩印包装纸箱厂新疆华盛坤包装有限公司新疆恒远中汇彩印包装有限公司新疆美包装有限公司石河子佳美包装工贸有限公司新疆彩源包装有限责任公司新疆亚泰隆纸制品包装有限公司新疆西成印刷包装有限公司铁门关市云广包装有限公司阿克苏江南宏达纸业有限公司阿克苏市方源彩印包装有限公司阿拉尔市峰诚包装有限公司乌鲁木齐科迪彩印包装有限公司恒安（昌吉）纸业有限公司新疆杉科技发展有限责任公司五家渠蔡家湖天利纸业有限公司巴州名星纸业有限责任公司哈密宏星木质素科技有限责任公司岳普湖益华纸业包装有限公司（来源：中华人民共和国信息和工业化部）历史文章精选包装用纸价格是明升暗降，还是易涨难跌？drupa 第5份全球趋势报告：迄今为止最棒的全球印刷业调查结果央视再揭秘：废纸价格淡季同比上涨近四成，正常吗?裕同一季度扣非净利润下跌近50%\n",
      "'gbk' codec can't encode character '\\u200b' in position 26: illegal multibyte sequence\n",
      "error:  ▲Google OKR 内部培训视频（北森翻译）截至现在，OKR已被全球数以千计的企业所采用，硅谷的一些知名企业如领英（LinkedIn）、推特（Twitter）和星佳（Zynga）对OKR更是大力推崇\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    不产生中间结果json文件，直接将分词结果写成word pos形式，保存在.txt文件中。避免从json中读取时出错。\n",
    "    \n",
    "    输入： testData文件夹下，38779.txt --- 42471.txt，成段的文本\n",
    "    \n",
    "    输出： testData_1.txt，word pos形式，粗糙 \n",
    "'''\n",
    "\n",
    "'''\n",
    "    提示： invalid literal for int() with base 10: ''\n",
    "    是因为有.ipynb_checkpoints，非空文件夹。\n",
    "    删除一次即可。\n",
    "    每次先检查一下是否有.ipynb_checkpoints\n",
    "'''\n",
    "\n",
    "\n",
    "import os\n",
    "# import shutil\n",
    "# shutil.rmtree(\"./testData/.ipynb_checkpoints\") \n",
    "\n",
    "import json\n",
    "import re\n",
    "total_number = 0\n",
    "output = open(\"./testData_1.txt\", \"w\", encoding=\"utf-8\")\n",
    "# f = open('justTest.json', 'w', encoding=\"utf-8\")\n",
    "for file in os.listdir(r'./testData'):\n",
    "    number = int(file.split('.')[0])\n",
    "    domain = os.path.abspath(r'./testData')\n",
    "    file = os.path.join(domain, file) \n",
    "    if 38779 <= number and number <= 42471: \n",
    "#         print(file)\n",
    "        data = open(file, \"r\", encoding=\"utf-8\")\n",
    "        title = ''\n",
    "        content = ''\n",
    "        para = []\n",
    "        for line in data:\n",
    "            line = line.strip()\n",
    "            try:\n",
    "                if line.startswith(\"title\"):\n",
    "                    title = line[6:]\n",
    "                if line.startswith(\"content\"):\n",
    "                    content = line[8:]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        text = title + \"。\" +content\n",
    "        text = re.split(\"([。])\", text)\n",
    "        for line in text:\n",
    "            para.append(line)\n",
    "#         f.write(json.dumps(posHtml(para),ensure_ascii=False,indent=2))\n",
    "        result = posHtml(para)\n",
    "        for sent in result:\n",
    "            for pair in sent:\n",
    "                word = pair.get(\"item\")\n",
    "                pos = pair.get(\"pos\")\n",
    "                if word == \"。\":\n",
    "                    output.write(word + \" \" + pos + \"\\r\\n\")\n",
    "                    output.write(\"\\n\")\n",
    "                else:\n",
    "                    output.write(word + \" \" + pos + \"\\r\\n\")\n",
    "#         total_number += 1\n",
    "                \n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "output.close()\n",
    "\n",
    "# print(\"total_number\", total_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    直接调用百度API之后的分词结果，可能由于英文，导致分出来的词是多个单词；\n",
    "    在形式上看就不是两列了，需要将除pos之外的列合并。\n",
    "    \n",
    "    输入： testData_1.txt, word pos形式，较粗糙，可能不止两列，\n",
    "    \n",
    "    输出： testDataNew_1.txt， word pos形式， 全部转成两列格式。\n",
    "'''\n",
    "\n",
    "cut_str = \"\"\n",
    "data_new = open(\"./testDataNew_1.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./testData_1.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        line = line.strip()\n",
    "        item = line.split(\" \")\n",
    "        l = len(line.split(\" \"))\n",
    "        if l == 2:\n",
    "            word = item[0]\n",
    "            pos = item[1]\n",
    "            data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "        if l > 2:\n",
    "            pos = item[-1]\n",
    "            for i in range(l-1):\n",
    "                if  not cut_str:\n",
    "                    tmp_str = item[i]\n",
    "                else:\n",
    "                    tmp_str = cut_str + \"$\" + item[i]\n",
    "                if i < l-1:\n",
    "                    cut_str = tmp_str\n",
    "            data_new.write(cut_str + \" \" + pos + \"\\r\\n\")\n",
    "            cut_str = \"\"\n",
    "            tmp_str = \"\"\n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    第一步：将所有拼接的word拆开，全部转成两列的word pos格式\n",
    "    \n",
    "    输入： testDataNew_1.txt, word（包括拼接）pos形式， 全部转成两列格式。\n",
    "    \n",
    "    输出： testDataNewAgain_1.txt， \n",
    "'''\n",
    "\n",
    "data_new = open(\"./testDataNewAgain_1.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./testDataNew_1.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        line = line.encode(\"utf-8\").decode(\"utf_8-sig\")\n",
    "        line = line.strip()\n",
    "        if len(line.split(\" \")) != 2:\n",
    "            print(line)\n",
    "            \n",
    "        if len(line.split(\" \")) == 2:\n",
    "            word = line.split(\" \")[0]\n",
    "            pos = line.split(\" \")[1]\n",
    "            \n",
    "        if len(word.split(\"$\")) == 1: # word不是拼接的\n",
    "            data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "            \n",
    "        if len(word.split(\"$\")) > 1: # word是拼接的\n",
    "            wordList = word.split(\"$\")\n",
    "            l = len(word.split(\"$\"))\n",
    "            for i in range(l):\n",
    "                data_new.write(wordList[i] + \" \" + pos + \"\\r\\n\")\n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n\n",
      "n\n",
      "nz\n",
      "nt\n",
      "nt\n",
      "nt\n",
      "nt\n",
      "nt\n",
      "nt\n",
      "nt\n",
      "nz\n",
      "nt\n",
      "nt\n",
      "nt\n",
      "nr\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "nt\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "nz\n",
      "m\n",
      "xc\n",
      "xc\n",
      "nz\n",
      "xc\n",
      "xc\n",
      "w\n",
      "w\n",
      "nz\n",
      "xc\n",
      "xc\n",
      "w\n",
      "w\n",
      "m\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "nt\n",
      "nz\n",
      "nr\n",
      "ns\n",
      "ns\n",
      "m\n",
      "m\n",
      "nw\n",
      "nw\n",
      "nz\n",
      "nt\n",
      "nt\n",
      "m\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "w\n",
      "w\n",
      "nr\n",
      "nr\n",
      "nr\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nr\n",
      "nr\n",
      "nz\n",
      "nr\n",
      "nz\n",
      "nr\n",
      "nt\n",
      "nr\n",
      "nr\n",
      "nz\n",
      "nz\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "xc\n",
      "xc\n",
      "w\n",
      "w\n",
      "nt\n",
      "nt\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nr\n",
      "nr\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nr\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nr\n",
      "nr\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nr\n",
      "nr\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nz\n",
      "nr\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "w\n",
      "xc\n",
      "xc\n",
      "nz\n",
      "nz\n",
      "xc\n",
      "xc\n",
      "nz\n",
      "nz\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "xc\n",
      "nz\n",
      "nz\n",
      "xc\n",
      "xc\n",
      "w\n",
      "w\n",
      "nz\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    第二步：如果pos不是nr nt ti，转换成o\n",
    "    \n",
    "    输入： testDataNewAgain_1.txt， word已经全部是拆开之后的格式\n",
    "    \n",
    "    输出： testDataNewAgain_2.txt \n",
    "    \n",
    "'''\n",
    "\n",
    "data_new = open(\"./testDataNewAgain_2.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./testDataNewAgain_1.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        line = line.encode(\"utf-8\").decode(\"utf_8-sig\")\n",
    "        line = line.strip()\n",
    "        if len(line.split(\" \")) != 2:\n",
    "            print(line)\n",
    "            \n",
    "        if len(line.split(\" \")) == 2:\n",
    "            word = line.split(\" \")[0]\n",
    "            pos = line.split(\" \")[1]\n",
    "            \n",
    "        if  pos == \"nt\" or pos ==\"nr\" or pos ==\"ti\":\n",
    "            \n",
    "            data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "        else:\n",
    "            data_new.write(word + \" \" + \"o\" + \"\\r\\n\")\n",
    "            \n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allIsChinese(text):\n",
    "    return all('\\u4e00' <= char <= '\\u9fff' for char in text)\n",
    "\n",
    "def isSymbol(text):\n",
    "    import string\n",
    "    punc = string.punctuation\n",
    "    punc = punc.replace(\".\", \"\")\n",
    "    return all(char in punc for char in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    第三步：将word转成单独的汉字或者单词，但是汉字/字母/数字/符号混合格式的数据暂未处理，一起保存下来\n",
    "    \n",
    "    输入： testDataNewAgain_2.txt， \n",
    "    \n",
    "    输出： testDataNewAgain_3.txt，\n",
    "    \n",
    "'''\n",
    "\n",
    "data_new = open(\"./testDataNewAgain_3.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./testDataNewAgain_2.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        line = line.encode(\"utf-8\").decode(\"utf_8-sig\")\n",
    "        line = line.strip()\n",
    "        if len(line.split(\" \")) != 2:\n",
    "            print(line)\n",
    "            \n",
    "        if len(line.split(\" \")) == 2:\n",
    "            word = line.split(\" \")[0]\n",
    "            pos = line.split(\" \")[1]\n",
    "        \n",
    "        if len(word) == 1:\n",
    "            data_new.write(word + \" \" + pos + \"\\r\\n\")  \n",
    "        \n",
    "        if len(word) > 1:\n",
    "            if allIsChinese(word): # 全是汉字\n",
    "                l = len(word)\n",
    "                for i in range(l):\n",
    "                    data_new.write(word[i] + \" \" + pos + \"\\r\\n\")   \n",
    "                    \n",
    "            if word.encode( 'UTF-8' ).isalpha():  # 全是字母           \n",
    "                data_new.write(word + \" \" + pos + \"\\r\\n\") \n",
    "            \n",
    "            if word.encode( 'UTF-8' ).isdigit():  # 全是数字           \n",
    "                data_new.write(word + \" \" + pos + \"\\r\\n\") \n",
    "                \n",
    "            if not allIsChinese(word) and not word.encode( 'UTF-8' ).isalpha() and not word.encode( 'UTF-8' ).isdigit():\n",
    "                data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "                \n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    第四步：将汉字/字母/数字/符号混合格式的数据处理，\n",
    "                ① 相邻元素类型不一致就划分（但小数被拆成了两个整数）\n",
    "    \n",
    "    输入： testDataNewAgain_3.txt，word转成单独的汉字或者单词，但是汉字/字母/数字/符号混合格式的数据暂未处理\n",
    "    \n",
    "    输出： testDataExp.txt，\n",
    "    \n",
    "'''\n",
    "\n",
    "data_new = open(\"./testDataExp.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./testDataNewAgain_3.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    tmp_eng = \"\"\n",
    "    tmp_num = \"\"\n",
    "    for line in data:\n",
    "        line = line.encode(\"utf-8\").decode(\"utf_8-sig\")\n",
    "        line = line.strip()\n",
    "        if len(line.split(\" \")) != 2:\n",
    "            print(1)\n",
    "            print(line)\n",
    "            \n",
    "        if len(line.split(\" \")) == 2:\n",
    "            word = line.split(\" \")[0]\n",
    "            pos = line.split(\" \")[1]\n",
    "            \n",
    "            if len(word) == 1:  # 如果word长度等于1，单独的汉字，字母，数字，标点符号等\n",
    "                data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "                \n",
    "            if len(word) > 1: # 如果word长度不小于1\n",
    "                \n",
    "                if allIsChinese(word): # 全是汉字\n",
    "                    l = len(word)\n",
    "                    for i in range(l):\n",
    "                        data_new.write(word[i] + \" \" + pos + \"\\r\\n\")  \n",
    "\n",
    "                if word.encode( 'UTF-8' ).isalpha():  # 全是字母           \n",
    "                    data_new.write(word + \" \" + pos + \"\\r\\n\") \n",
    "                \n",
    "\n",
    "                if word.encode( 'UTF-8' ).isdigit():  # 全是数字           \n",
    "                    data_new.write(word + \" \" + pos + \"\\r\\n\") \n",
    "                    \n",
    "                if not allIsChinese(word) and not word.encode( 'UTF-8' ).isalpha() and not word.encode( 'UTF-8' ).isdigit():\n",
    "            \n",
    "                    for i in range(len(word)):\n",
    "\n",
    "                        if word[i] in tmp_eng and word[i-1].encode('utf-8').isalpha():\n",
    "                            continue\n",
    "\n",
    "                        if word[i] in tmp_num and word[i-1].encode('utf-8').isdigit():\n",
    "                            continue\n",
    "\n",
    "                        if word[i].encode( 'UTF-8' ).isalpha():\n",
    "                            eng = word[i]\n",
    "                            for j in range(i+1, len(word)):\n",
    "                                if j < len(word):\n",
    "                                    if word[j].encode('utf-8').isalpha():\n",
    "                                        eng  = eng + word[j]\n",
    "                                    else: # 遇到相邻的字符，类型不一致\n",
    "                                        data_new.write(eng + \" \" + pos + \"\\r\\n\")\n",
    "                                        tmp_eng = eng\n",
    "                                        eng = \"\"\n",
    "                                        break\n",
    "                            else:\n",
    "                                data_new.write(eng + \" \" + pos + \"\\r\\n\") # 相邻的字符类型都一致，并且把字符串遍历完了。\n",
    "                                tmp_eng = eng\n",
    "                                eng = \"\"\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        if word[i].encode( 'UTF-8' ).isdigit():\n",
    "                            num = word[i]\n",
    "                            for j in range(i+1, len(word)):\n",
    "                                if j < len(word):\n",
    "                                    if word[j].encode('utf-8').isdigit():\n",
    "                                        num  = num + word[j]     \n",
    "                                    else:\n",
    "                                        data_new.write(num + \" \" + pos + \"\\r\\n\")\n",
    "                                        tmp_num = num\n",
    "                                        num = \"\"\n",
    "                                        break\n",
    "                            else:\n",
    "                                data_new.write(num + \" \" + pos + \"\\r\\n\")\n",
    "                                tmp_num = num\n",
    "                                num = \"\"\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        if allIsChinese(word[i]):\n",
    "                            data_new.write(word[i] + \" \" + pos + \"\\r\\n\")\n",
    "\n",
    "                        if isSymbol(word[i]):\n",
    "                            data_new.write(word[i] + \" \" + pos + \"\\r\\n\") \n",
    "                            \n",
    "\n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
