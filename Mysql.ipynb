{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从mysql中读取数据，用百度接口分词，转成一行一个字一个词性的格式，交给CRF++预测（预测这一步直接在服务器上操作，download目录下的CRF++中），再将CRF++预测的结果跟百度预测的结果做个统计（主要统计词性是ti的词）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'source', 'es_id', 'pub_time', 'abstract', 'create_time', 'update_time', 'status', 'platform', 'source_id', 'es_index']\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "# 表名：corpus.title_ner\n",
    "\n",
    "conn = pymysql.connect(host=\"***.***.**.***\", user =\"*******\", password=\"**********\",\n",
    "                    database=\"*******\", port=3306, charset=\"utf8\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = \"SELECT * FROM news\"\n",
    "\n",
    "cursor.execute(sql)\n",
    "\n",
    "col_name_list = [tuple[0] for tuple in cursor.description]\n",
    "\n",
    "# result = cursor.execute(\"select * from corpus.title_ner limit 1;\")\n",
    "\n",
    "print(col_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1610"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor.execute(\"select distinct source from news where platform='微信公众号' and status =0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'source', 'es_id', 'pub_time', 'abstract', 'create_time', 'update_time', 'status', 'platform', 'source_id', 'es_index']\n"
     ]
    }
   ],
   "source": [
    "import pymysql\n",
    "\n",
    "conn = pymysql.connect(host=\"***.***.**.***\", user =\"*******\", password=\"**********\",\n",
    "                    database=\"*******\", port=3306, charset=\"utf8\")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "sql = \"SELECT * FROM bailian.news\"\n",
    "\n",
    "cursor.execute(sql)\n",
    "\n",
    "col_name_list = [tuple[0] for tuple in cursor.description]\n",
    "\n",
    "# result = cursor.execute(\"select * from corpus.title_ner limit 1;\")\n",
    "\n",
    "print(col_name_list)\n",
    "# MYSQL_DEVELOPMENT = {\n",
    "#     'host': '123.206.13.101',\n",
    "#     'port': 3306,\n",
    "#     'db': 'bailian',\n",
    "#     'user': 'apiuser0',\n",
    "#     'passwd': 'drQMs0KLlT$',\n",
    "#     'charset': 'utf8',\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencesMaker(html):\n",
    "\n",
    "    import justext\n",
    "    paragraphs = justext.justext(html, [])\n",
    "\n",
    "    cache_sentences = ''\n",
    "    sentences = []\n",
    "\n",
    "\n",
    "    for p in paragraphs:\n",
    "        sent = p.text.strip().replace('\\xa0', '').replace('\\u3000', '')\n",
    "        sent = sent.encode('gb2312', 'ignore').decode('gb2312').encode('gbk', 'ignore').decode('gbk')\n",
    "        if not sent:\n",
    "            continue\n",
    "\n",
    "        # 可能是含有名字，需要进一步处理\n",
    "        if len(cache_sentences) < 5:\n",
    "            cache_sentences += ' ' + sent\n",
    "\n",
    "        else:\n",
    "            sentences.append(cache_sentences.strip())\n",
    "            cache_sentences = sent\n",
    "\n",
    "    if not not cache_sentences:\n",
    "        sentences.append(cache_sentences.strip())\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"link.csv.xlsx\")\n",
    "\n",
    "output = open(\"./content.txt\", \"w\", encoding=\"utf-8\")\n",
    "\n",
    "for i in range(len(data)):\n",
    "    url_name = data.values[i][1]\n",
    "    cursor.execute(\"select content from corpus.title_ner where url = '%s' \" %(url_name))\n",
    "    result = cursor.fetchall()\n",
    "    output.write(\"\".join(sentencesMaker(result[0][0])) + \"\\r\\n\")\n",
    "    \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aip import AipNlp\n",
    "\n",
    "APP_ID = '********'\n",
    "API_KEY = '********************'\n",
    "SECRET_KEY = '*****************************'\n",
    "\n",
    "client = AipNlp(APP_ID, API_KEY, SECRET_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************************************\n",
      "***********************************************\n",
      "***********************************************\n",
      "***********************************************\n",
      "***********************************************\n",
      "***********************************************\n",
      "***********************************************\n",
      "***********************************************\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"data.json\", \"w\")\n",
    "for i in ulist:\n",
    "    f.write(json.dumps(client.lexerCustom(i[0]),ensure_ascii=False,indent=2))\n",
    "    print(\"***********************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改善QPS限制\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "SPLIT_LINE_MARKER = '！'\n",
    "SPLIT_LINE_MARKER_SIZE = 3\n",
    "\n",
    "# 拆分为句子\n",
    "\n",
    "\n",
    "# 重新恢复句子\n",
    "def restoreSentences(text, only_per=False):  # 长度不超过3700字节的句子\n",
    "    restore_sentences = []\n",
    "    isSucc = False\n",
    "    if text is None:\n",
    "        return restore_sentences, isSucc\n",
    "\n",
    "    result = client.lexerCustom(text)\n",
    "\n",
    "    items = result.get('items', [])\n",
    "    items_size = len(items)\n",
    "\n",
    "    tries_limit = 3\n",
    "    tries_counter = 0\n",
    "    \n",
    "    while items_size == 0: # 分词结果为空\n",
    "        if len(text) != 0: # 但句子长度不为空\n",
    "            # 可能是qps限制\n",
    "            time.sleep(1)\n",
    "            result = client.lexerCustom(text)\n",
    "\n",
    "            items = result.get('items', [])\n",
    "            items_size = len(items)\n",
    "            isSucc=True\n",
    "\n",
    "        tries_counter += 1\n",
    "\n",
    "        if tries_counter >= tries_limit: # 分词尝试大于等于4次之后仍失败\n",
    "            print(f'error: 分词api请求失败多次！{text}')\n",
    "#             print('error: 分词api请求失败多次!')\n",
    "            return restore_sentences, isSucc\n",
    "\n",
    "    restore_idx = 0\n",
    "\n",
    "    last_restore_idx = 0\n",
    "    has_per = False\n",
    "\n",
    "    while restore_idx < items_size: # 对每个分词结果进行整理 \n",
    "        # 分词不是拼接符\"!!!\"\n",
    "        while restore_idx < items_size and items[restore_idx]['item'] != SPLIT_LINE_MARKER:\n",
    "            item = items[restore_idx] # 先把第一个分词的结果(dict)赋值给item，item整理之后再直接赋给原items[]\n",
    "            # TODO 剔除机构中的不合法字符\n",
    "            format_pos = item['pos']\n",
    "            \n",
    "            # 对非ne标识的分词不做处理\n",
    "            \n",
    "            if item['ne'].startswith('ORG'): # 如果该分词的ne是ORG\n",
    "                invalid_orgs = ['公司']\n",
    "                item['item'] = item['item'].replace('&', '')\n",
    "                if item['item'] in invalid_orgs:\n",
    "                    format_pos = 'n' # 普通名词\n",
    "                else:\n",
    "                    format_pos = 'nt'  # 机构团体名\n",
    "\n",
    "            elif item['ne'] == 'PER': # 如果该分词的ne是PER\n",
    "                format_pos = 'nr' # 人名\n",
    "\n",
    "            elif item['ne'] == 'TITLE': # 如果该分词的ne是TITLE（定制）\n",
    "                format_pos = 'ti' # 职称\n",
    "\n",
    "            elif item['ne'] == 'LOC': # 如果该分词的ne是LOC\n",
    "                format_pos = 'ns'  # 地名\n",
    "\n",
    "            elif item['ne'] == 'TIME': # 如果该分词的ne是TIME\n",
    "                format_pos = 't' # 时间名词\n",
    "                \n",
    "                \n",
    "\n",
    "            if format_pos == '': #如果pos为空，即是其他非上述的ne标识，将pos置为\"xx\"\n",
    "                format_pos = 'xx'\n",
    "\n",
    "            elif format_pos == 'nr':\n",
    "                # 过滤先生或者女士之类的名称\n",
    "                name = re.sub(r'((先生)|(小姐)|(阿姨)|(叔叔)|(女士)|(同志)|总)$', '', item['item'])\n",
    "\n",
    "                if len(name) >= 2:\n",
    "                    invalid_names = {\n",
    "                        '区块链': 'n'\n",
    "                    }\n",
    "\n",
    "                    if name not in invalid_names:\n",
    "                        has_per = True\n",
    "                        item['item'] = name\n",
    "                    else:\n",
    "                        format_pos = invalid_names[name]\n",
    "\n",
    "                else: # 剔除称谓之后的name长度如果小于2，就不是nr，设为n\n",
    "\n",
    "                     format_pos = 'n'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            item['pos'] = format_pos # 处理之后的pos赋值给原分词的pos\n",
    "\n",
    "            # 删除无用信息\n",
    "            item.pop('basic_words')\n",
    "            item.pop('formal')\n",
    "            item.pop('byte_length')\n",
    "            item.pop('byte_offset')\n",
    "            item.pop('loc_details')\n",
    "            item.pop('ne')\n",
    "            item.pop('uri')\n",
    "\n",
    "            items[restore_idx] = item # 处理之后的分词结果赋给原分词结果\n",
    "            restore_idx += 1 # 继续下一个item\n",
    "\n",
    "            \n",
    "        # 若遇到了拼接符\"!!!\"或对所有非拼接符的分词结果处理完毕    \n",
    "        if restore_idx + SPLIT_LINE_MARKER_SIZE - 1 < items_size: # 如果不是句末的最后一个拼接符\n",
    "            needCut = True # 将修改之后的分词结果仍以一个sent为单位cut\n",
    "            for i in range(SPLIT_LINE_MARKER_SIZE - 1): # 再次判断后一位是否是拼接符，如不是，不需要cut\n",
    "                if items[restore_idx + i + 1]['item'] != SPLIT_LINE_MARKER:\n",
    "                    needCut = False\n",
    "                    break\n",
    "\n",
    "            if needCut:\n",
    "                ed = max(restore_idx, 0)\n",
    "\n",
    "                sentence_items = items[last_restore_idx:ed] # 切句子，[0:restore_idx]\n",
    "                if len(sentence_items) != 0 and (has_per or not only_per):\n",
    "                    # print('per:', sentence_items)\n",
    "                    restore_sentences.append(sentence_items)\n",
    "\n",
    "                next_st = min(ed + SPLIT_LINE_MARKER_SIZE, items_size)\n",
    "                last_restore_idx = next_st\n",
    "\n",
    "                restore_idx += SPLIT_LINE_MARKER_SIZE\n",
    "            else:\n",
    "                restore_idx += 1\n",
    "\n",
    "        else: # 句末最后一个拼接符\n",
    "            ed = max(restore_idx, 0)\n",
    "            sentence_items = items[last_restore_idx:ed]\n",
    "            if len(sentence_items) != 0 and (has_per or not only_per):\n",
    "\n",
    "                restore_sentences.append(sentence_items)\n",
    "\n",
    "            restore_idx = items_size\n",
    "\n",
    "        has_per = False\n",
    "\n",
    "    # print(restore_sentences)\n",
    "    return restore_sentences, isSucc\n",
    "\n",
    "\n",
    "# 解析并标注HTML\n",
    "def posHtml(sentences, only_per=False):\n",
    "#     sentences = sentencesMaker(html) # 将带有html标签的段落整理成一个list,去除标签的文字段落。\n",
    "\n",
    "\n",
    "    cut_str = '' \n",
    "\n",
    "    pos_sentences = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        sent = sent.strip()\n",
    "        if not sent:  # 若句子为空，跳出循环\n",
    "            continue\n",
    "        if not cut_str: # 若cut_str为空，即第一句之前，把第一句话赋值给tmp_str\n",
    "            tmp_str = sent\n",
    "        else: # 若cut_str里有句子，将后续句子拼接，以“!!!”作为拼接符，再赋值给tmp_str\n",
    "            tmp_str = cut_str + SPLIT_LINE_MARKER * SPLIT_LINE_MARKER_SIZE + sent\n",
    "        if sys.getsizeof(tmp_str) < 3700: # 拼接之后的句子小于3700字节\n",
    "            cut_str = tmp_str # tmp_str赋值给cut_str，继续拼接sent\n",
    "        else:\n",
    "            try:\n",
    "                if cut_str:\n",
    "                    time.sleep(0.5) # 拼接之后的tmp_str若大于3700字节，取未拼接该sent之前的cut_str拿来分词\n",
    "                    sents, issucc = restoreSentences(cut_str, only_per)\n",
    "                    pos_sentences += sents\n",
    "    #                 if not issucc:\n",
    "    #                     print(html)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print('error: ', sent)\n",
    "\n",
    "            cut_str = sent # 把刚才没有拼接成功的sent重新赋给cut_str\n",
    "\n",
    "\n",
    "    if not not cut_str: # 最后一句sent\n",
    "        time.sleep(0.5)\n",
    "        try:\n",
    "            if cut_str:\n",
    "                time.sleep(0.5) # 拼接之后的tmp_str若大于3700字节，取未拼接该sent之前的cut_str拿来分词\n",
    "                sents, issucc = restoreSentences(cut_str, only_per)\n",
    "                pos_sentences += sents\n",
    "#                 if not issucc:\n",
    "#                     print(html)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('error: ', sent)\n",
    "\n",
    "    return pos_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    接着将获取的link的全部content拿来给百度API分词\n",
    "    注意，现在的格式是每行是一篇文章，可能会过长；\n",
    "    先将每篇文章每句话拼一起，再将所有文章的每句话拼一起；\n",
    "    所有的结果放在content_list这个list中\n",
    "'''\n",
    "import re\n",
    "output = open(\"./0_cutWord.txt\", \"w\", encoding=\"utf-8\")\n",
    "data = open(\"./0.txt\", \"r\", encoding=\"utf-8\")\n",
    "# sent_list = []\n",
    "content_list = []\n",
    "for line in data:\n",
    "    line = line.strip()\n",
    "    line = re.split(\"([。])\", line)\n",
    "    for sent in line:\n",
    "        content_list.append(sent)\n",
    "\n",
    "result = posHtml(content_list)\n",
    "for sent in result:\n",
    "    for pair in sent:\n",
    "        word = pair.get(\"item\")\n",
    "        pos = pair.get(\"pos\")\n",
    "        output.write(word + \" \" + pos + \"\\r\\n\")\n",
    "    \n",
    "data.close()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    直接调用百度API之后的分词结果，可能由于英文，导致分出来的词是多个单词；\n",
    "    在形式上看就不是两列了，需要将除pos之外的列合并。\n",
    "    \n",
    "    输入：  word pos形式，较粗糙，由于英文单词，符号等，可能不止两列，\n",
    "    \n",
    "    输出：   word pos形式， 全部转成两列格式。\n",
    "'''\n",
    "\n",
    "cut_str = \"\"\n",
    "data_new = open(\"./0_cutWord_1.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./0_cutWord.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        line = line.strip()\n",
    "        item = line.split(\" \")\n",
    "        l = len(line.split(\" \"))\n",
    "        if l == 2:\n",
    "            word = item[0]\n",
    "            pos = item[1]\n",
    "            data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "        if l > 2:\n",
    "            pos = item[-1]\n",
    "            for i in range(l-1):\n",
    "                if  not cut_str:\n",
    "                    tmp_str = item[i]\n",
    "                else:\n",
    "                    tmp_str = cut_str + \"$\" + item[i]\n",
    "                if i < l-1:\n",
    "                    cut_str = tmp_str\n",
    "            data_new.write(cut_str + \" \" + pos + \"\\r\\n\")\n",
    "            cut_str = \"\"\n",
    "            tmp_str = \"\"\n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allIsChinese(text):\n",
    "    return all('\\u4e00' <= char <= '\\u9fff' for char in text)\n",
    "\n",
    "def isSymbol(text):\n",
    "    import string\n",
    "    punc = string.punctuation\n",
    "    punc = punc.replace(\".\", \"\") # 去掉\".\"，因为数据里可能有小数存在，而且格式不固定，正则不太好匹配，所以先不处理。\n",
    "    return all(char in punc for char in text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    第一步：将所有拼接的word拆开，全部转成两列的word pos格式\n",
    "    \n",
    "    输入： word（包括拼接）pos形式， 全部转成两列格式。\n",
    "    \n",
    "    输出： word（仅有一列）pos形式 \n",
    "'''\n",
    "\n",
    "data_new = open(\"./0_cutWord_2.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./0_cutWord_1.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        line = line.encode(\"utf-8\").decode(\"utf_8-sig\")\n",
    "        line = line.strip()\n",
    "        if len(line.split(\" \")) != 2:\n",
    "            print(line)\n",
    "            \n",
    "        if len(line.split(\" \")) == 2:\n",
    "            word = line.split(\" \")[0]\n",
    "            pos = line.split(\" \")[1]\n",
    "            \n",
    "        if len(word.split(\"$\")) == 1: # word不是拼接的\n",
    "            data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "            \n",
    "        if len(word.split(\"$\")) > 1: # word是拼接的\n",
    "            wordList = word.split(\"$\")\n",
    "            l = len(word.split(\"$\"))\n",
    "            for i in range(l):\n",
    "                data_new.write(wordList[i] + \" \" + pos + \"\\r\\n\")\n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    第二步：如果pos不是nr nt ti，转换成o\n",
    "    \n",
    "    输入： word pos， word已经全部是拆开之后的格式\n",
    "    \n",
    "    输出： word o/ word nr/ word nt/ word ti\n",
    "    \n",
    "'''\n",
    "\n",
    "data_new = open(\"./0_cutWord_3.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./0_cutWord_2.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        line = line.encode(\"utf-8\").decode(\"utf_8-sig\")\n",
    "        line = line.strip()\n",
    "        if len(line.split(\" \")) != 2:\n",
    "            continue\n",
    "#             print(line)\n",
    "            \n",
    "        if len(line.split(\" \")) == 2:\n",
    "            word = line.split(\" \")[0]\n",
    "            pos = line.split(\" \")[1]\n",
    "            \n",
    "        if  pos == \"nt\" or pos ==\"nr\" or pos ==\"ti\":\n",
    "            \n",
    "            data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "        else:\n",
    "            data_new.write(word + \" \" + \"o\" + \"\\r\\n\")\n",
    "            \n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    第三步：将word转成单独的汉字或者单词，但是汉字/字母/数字/符号混合格式的数据暂未处理，一起保存下来\n",
    "    \n",
    "    输入： 汉字汉字 pos \n",
    "    \n",
    "    输出： 字 pos\n",
    "    \n",
    "'''\n",
    "import string\n",
    "punc = string.punctuation\n",
    "data_new = open(\"./0_cutWord_4.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./0_cutWord_3.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        line = line.encode(\"utf-8\").decode(\"utf_8-sig\")\n",
    "        line = line.strip()\n",
    "        if len(line.split(\" \")) != 2:\n",
    "            print(line)\n",
    "            \n",
    "        if len(line.split(\" \")) == 2:\n",
    "            word = line.split(\" \")[0]\n",
    "            pos = line.split(\" \")[1]\n",
    "        \n",
    "        if len(word) == 1:\n",
    "            data_new.write(word + \" \" + pos + \"\\r\\n\")  \n",
    "        \n",
    "        if len(word) > 1:\n",
    "            if allIsChinese(word): # 全是汉字\n",
    "                l = len(word)\n",
    "                for i in range(l):\n",
    "                    data_new.write(word[i] + \" \" + pos + \"\\r\\n\")   \n",
    "                    \n",
    "            if word.encode( 'UTF-8' ).isalpha():  # 全是字母           \n",
    "                data_new.write(word + \" \" + pos + \"\\r\\n\") \n",
    "            \n",
    "            if word.encode( 'UTF-8' ).isdigit():  # 全是数字           \n",
    "                data_new.write(word + \" \" + pos + \"\\r\\n\") \n",
    "                \n",
    "            if not allIsChinese(word) and not word.encode( 'UTF-8' ).isalpha() and not word.encode( 'UTF-8' ).isdigit():\n",
    "                data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "                \n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    第四步：将汉字/字母/数字/符号混合格式的数据处理，\n",
    "                ① 相邻元素类型不一致就划分（但小数被拆成了两个整数）\n",
    "    \n",
    "    输入： word转成单独的汉字或者单词，但是汉字/字母/数字/符号混合格式的数据暂未处理\n",
    "    \n",
    "    输出： \n",
    "    \n",
    "'''\n",
    "tmp_eng = \"\"\n",
    "tmp_num = \"\"\n",
    "\n",
    "data_new = open(\"./0_Exp.txt\", \"w\", encoding=\"utf-8\")\n",
    "with open(\"./0_cutWord_4.txt\", \"r\", encoding=\"utf-8\") as data:\n",
    "    for line in data:\n",
    "        line = line.encode(\"utf-8\").decode(\"utf_8-sig\")\n",
    "        line = line.strip()\n",
    "        if len(line.split(\" \")) != 2:\n",
    "            print(1)\n",
    "            print(line)\n",
    "            \n",
    "        if len(line.split(\" \")) == 2:\n",
    "            word = line.split(\" \")[0]\n",
    "            pos = line.split(\" \")[1]\n",
    "            \n",
    "            if len(word) == 1:  # 如果word长度等于1，单独的汉字，字母，数字，标点符号等\n",
    "                data_new.write(word + \" \" + pos + \"\\r\\n\")\n",
    "                \n",
    "            if len(word) > 1: # 如果word长度不小于1\n",
    "                \n",
    "                if allIsChinese(word): # 全是汉字\n",
    "                    l = len(word)\n",
    "                    for i in range(l):\n",
    "                        data_new.write(word[i] + \" \" + pos + \"\\r\\n\")  \n",
    "\n",
    "                if word.encode( 'UTF-8' ).isalpha():  # 全是字母           \n",
    "                    data_new.write(word + \" \" + pos + \"\\r\\n\") \n",
    "                \n",
    "\n",
    "                if word.encode( 'UTF-8' ).isdigit():  # 全是数字           \n",
    "                    data_new.write(word + \" \" + pos + \"\\r\\n\") \n",
    "                    \n",
    "                if not allIsChinese(word) and not word.encode( 'UTF-8' ).isalpha() and not word.encode( 'UTF-8' ).isdigit():\n",
    "            \n",
    "                    for i in range(len(word)):\n",
    "\n",
    "                        if word[i] in tmp_eng and word[i-1].encode('utf-8').isalpha():\n",
    "                            continue\n",
    "\n",
    "                        if word[i] in tmp_num and word[i-1].encode('utf-8').isdigit():\n",
    "                            continue\n",
    "\n",
    "                        if word[i].encode( 'UTF-8' ).isalpha():\n",
    "                            eng = word[i]\n",
    "                            for j in range(i+1, len(word)):\n",
    "                                if j < len(word):\n",
    "                                    if word[j].encode('utf-8').isalpha():\n",
    "                                        eng  = eng + word[j]\n",
    "                                    else: # 遇到相邻的字符，类型不一致\n",
    "                                        data_new.write(eng + \" \" + pos + \"\\r\\n\")\n",
    "                                        tmp_eng = eng\n",
    "                                        eng = \"\"\n",
    "                                        break\n",
    "                            else:\n",
    "                                data_new.write(eng + \" \" + pos + \"\\r\\n\") # 相邻的字符类型都一致，并且把字符串遍历完了。\n",
    "                                tmp_eng = eng\n",
    "                                eng = \"\"\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        if word[i].encode( 'UTF-8' ).isdigit():\n",
    "                            num = word[i]\n",
    "                            for j in range(i+1, len(word)):\n",
    "                                if j < len(word):\n",
    "                                    if word[j].encode('utf-8').isdigit():\n",
    "                                        num  = num + word[j]     \n",
    "                                    else:\n",
    "                                        data_new.write(num + \" \" + pos + \"\\r\\n\")\n",
    "                                        tmp_num = num\n",
    "                                        num = \"\"\n",
    "                                        break\n",
    "                            else:\n",
    "                                data_new.write(num + \" \" + pos + \"\\r\\n\")\n",
    "                                tmp_num = num\n",
    "                                num = \"\"\n",
    "                            continue\n",
    "\n",
    "\n",
    "                        if allIsChinese(word[i]):\n",
    "                            data_new.write(word[i] + \" \" + pos + \"\\r\\n\")\n",
    "\n",
    "                        if isSymbol(word[i]):\n",
    "                            data_new.write(word[i] + \" \" + pos + \"\\r\\n\") \n",
    "                            \n",
    "\n",
    "data_new.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读取CRF++的预测结果，转成一行三列的格式，word, true_label, pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    这次写出来的结果，三列数据写成了两行，第一行是 word \\t truePos，第二行是predictedPos。\n",
    "    \n",
    "    不知为啥格式变了。\n",
    "'''\n",
    "new_result = open('./resultOfContentUseCorpus3TiLabellll.txt','w',encoding='utf-8')\n",
    "output = open('./resultOfContentUseCorpus3TiLabel.txt','r',encoding='utf-8')\n",
    "pairList = []\n",
    "for line in output:\n",
    "    line = line.encode('utf-8').decode('utf-8-sig').strip()\n",
    "    if line:\n",
    "        if \"\\t\" in line:\n",
    "            line = line.split(\"\\t\")\n",
    "            tmp_1 = line\n",
    "        else:\n",
    "            pair = tmp_1[0] + \" \" + tmp_1[1] + \" \" +line\n",
    "            new_result.write(tmp_1[0] + \" \" + tmp_1[1] + \" \" + line + \"\\r\\n\")\n",
    "            pairList.append(pair)\n",
    "output.close()\n",
    "new_result.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "/data/wjc/jupyter/es/tag/24.csv.xlsx\n",
      "2\n",
      "/data/wjc/jupyter/es/tag/26.csv.xlsx\n",
      "3\n",
      "/data/wjc/jupyter/es/tag/23.csv.xlsx\n",
      "4\n",
      "/data/wjc/jupyter/es/tag/34.csv.xlsx\n",
      "5\n",
      "/data/wjc/jupyter/es/tag/38.csv.xlsx\n",
      "6\n",
      "/data/wjc/jupyter/es/tag/41.csv.xlsx\n",
      "7\n",
      "/data/wjc/jupyter/es/tag/42.csv.xlsx\n",
      "8\n",
      "/data/wjc/jupyter/es/tag/43.csv.xlsx\n",
      "9\n",
      "/data/wjc/jupyter/es/tag/44.csv.xlsx\n",
      "10\n",
      "/data/wjc/jupyter/es/tag/45.csv.xlsx\n",
      "11\n",
      "/data/wjc/jupyter/es/tag/46.csv.xlsx\n",
      "12\n",
      "/data/wjc/jupyter/es/tag/47.csv.xlsx\n",
      "13\n",
      "/data/wjc/jupyter/es/tag/39.csv.xlsx\n",
      "14\n",
      "/data/wjc/jupyter/es/tag/40.csv.xlsx\n",
      "15\n",
      "/data/wjc/jupyter/es/tag/49.csv.xlsx\n",
      "16\n",
      "/data/wjc/jupyter/es/tag/50.csv.xlsx\n",
      "17\n",
      "/data/wjc/jupyter/es/tag/51.csv.xlsx\n",
      "18\n",
      "/data/wjc/jupyter/es/tag/53.csv.xlsx\n",
      "19\n",
      "/data/wjc/jupyter/es/tag/54.csv.xlsx\n",
      "20\n",
      "/data/wjc/jupyter/es/tag/57.csv.xlsx\n",
      "21\n",
      "/data/wjc/jupyter/es/tag/58.csv.xlsx\n",
      "22\n",
      "/data/wjc/jupyter/es/tag/59.csv.xlsx\n",
      "23\n",
      "/data/wjc/jupyter/es/tag/60.csv.xlsx\n",
      "24\n",
      "/data/wjc/jupyter/es/tag/62.csv.xlsx\n",
      "25\n",
      "/data/wjc/jupyter/es/tag/63.csv.xlsx\n",
      "26\n",
      "/data/wjc/jupyter/es/tag/64.csv.xlsx\n",
      "27\n",
      "/data/wjc/jupyter/es/tag/66.csv.xlsx\n",
      "28\n",
      "/data/wjc/jupyter/es/tag/67.csv.xlsx\n",
      "29\n",
      "/data/wjc/jupyter/es/tag/68.csv.xlsx\n",
      "30\n",
      "/data/wjc/jupyter/es/tag/70.csv.xlsx\n",
      "31\n",
      "/data/wjc/jupyter/es/tag/71.csv.xlsx\n",
      "32\n",
      "/data/wjc/jupyter/es/tag/73.csv.xlsx\n",
      "33\n",
      "/data/wjc/jupyter/es/tag/74.csv.xlsx\n",
      "34\n",
      "/data/wjc/jupyter/es/tag/75.csv.xlsx\n",
      "35\n",
      "/data/wjc/jupyter/es/tag/76.csv.xlsx\n",
      "36\n",
      "/data/wjc/jupyter/es/tag/77.csv.xlsx\n",
      "37\n",
      "/data/wjc/jupyter/es/tag/78.csv.xlsx\n",
      "38\n",
      "/data/wjc/jupyter/es/tag/80.csv.xlsx\n",
      "39\n",
      "/data/wjc/jupyter/es/tag/82.csv.xlsx\n",
      "40\n",
      "/data/wjc/jupyter/es/tag/84.csv.xlsx\n",
      "41\n",
      "/data/wjc/jupyter/es/tag/85.csv.xlsx\n",
      "42\n",
      "/data/wjc/jupyter/es/tag/86.csv.xlsx\n",
      "43\n",
      "/data/wjc/jupyter/es/tag/87.csv.xlsx\n",
      "44\n",
      "/data/wjc/jupyter/es/tag/88.csv.xlsx\n",
      "45\n",
      "/data/wjc/jupyter/es/tag/89.csv.xlsx\n",
      "46\n",
      "/data/wjc/jupyter/es/tag/90.csv.xlsx\n",
      "47\n",
      "/data/wjc/jupyter/es/tag/92.csv.xlsx\n",
      "48\n",
      "/data/wjc/jupyter/es/tag/93.csv.xlsx\n",
      "49\n",
      "/data/wjc/jupyter/es/tag/94.csv.xlsx\n",
      "50\n",
      "/data/wjc/jupyter/es/tag/95.csv.xlsx\n",
      "51\n",
      "/data/wjc/jupyter/es/tag/96.csv.xlsx\n",
      "52\n",
      "/data/wjc/jupyter/es/tag/97.csv.xlsx\n",
      "53\n",
      "/data/wjc/jupyter/es/tag/99.csv.xlsx\n",
      "54\n",
      "/data/wjc/jupyter/es/tag/100.csv.xlsx\n",
      "55\n",
      "/data/wjc/jupyter/es/tag/102.csv.xlsx\n",
      "56\n",
      "/data/wjc/jupyter/es/tag/104.csv.xlsx\n",
      "57\n",
      "/data/wjc/jupyter/es/tag/105.csv.xlsx\n",
      "58\n",
      "/data/wjc/jupyter/es/tag/107.csv.xlsx\n",
      "59\n",
      "/data/wjc/jupyter/es/tag/108.csv.xlsx\n",
      "60\n",
      "/data/wjc/jupyter/es/tag/111.csv.xlsx\n",
      "61\n",
      "/data/wjc/jupyter/es/tag/112.csv.xlsx\n",
      "62\n",
      "/data/wjc/jupyter/es/tag/114.csv.xlsx\n",
      "63\n",
      "/data/wjc/jupyter/es/tag/116.csv.xlsx\n",
      "64\n",
      "/data/wjc/jupyter/es/tag/117.csv.xlsx\n",
      "65\n",
      "/data/wjc/jupyter/es/tag/8.csv.xlsx\n",
      "66\n",
      "/data/wjc/jupyter/es/tag/12.csv.xlsx\n",
      "67\n",
      "/data/wjc/jupyter/es/tag/14.csv.xlsx\n",
      "68\n",
      "/data/wjc/jupyter/es/tag/119.csv.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# import shutil\n",
    "# shutil.rmtree(\"/tag/.DS_Store\")\n",
    "i = 1\n",
    "for file in os.listdir(r'./tag'):\n",
    "    number = file.split('.')[0]\n",
    "    if number:\n",
    "        number = int(file.split('.')[0])\n",
    "        domain = os.path.abspath(r'./tag')\n",
    "        file = os.path.join(domain, file) \n",
    "        if pd.read_excel(file).empty:\n",
    "            print(i)\n",
    "            print(file)\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "head() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-254-dfce0e05481d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: head() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "i = 1\n",
    "output = open(\"./tag_result.txt\", \"w\", encoding=\"utf_8\")\n",
    "for file in os.listdir(r'./tag'):\n",
    "    number = file.split('.')[0]\n",
    "    if number:\n",
    "        number = int(file.split('.')[0])\n",
    "        domain = os.path.abspath(r'./tag')\n",
    "        file = os.path.join(domain, file)\n",
    "        if not pd.read_excel(file).empty:\n",
    "            df = pd.DataFrame(pd.read_excel(file).head(1))\n",
    "            print(i)\n",
    "            print(file)\n",
    "            print(df)\n",
    "            output.write(str(df))\n",
    "            print(\"***************************************************\")\n",
    "            i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试CER++的预测效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisionRate_ti: 0.7692307692307693\n",
      "recallRate_ti: 0.6666666666666666\n",
      "F_ti: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "recallTotal_ti = 0\n",
    "precisionTotal_ti = 0\n",
    "correctTotal_ti = 0\n",
    "\n",
    "with open(\"./resultOf0UseTestdata1.txt\", \"r\", encoding=\"utf-8\") as output:\n",
    "    for line in output:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            pair = line.split(\" \")\n",
    "            if pair[1] == 'ti':\n",
    "                recallTotal_ti += 1\n",
    "\n",
    "            if pair[2] == 'ti':\n",
    "                precisionTotal_ti += 1\n",
    "\n",
    "            if pair[1] == pair[2] and pair[1]== 'ti':\n",
    "                correctTotal_ti += 1  \n",
    "\n",
    "precisionRate_ti = 1.0 * correctTotal_ti / precisionTotal_ti\n",
    "recallRate_ti = 1.0 * correctTotal_ti / recallTotal_ti\n",
    "F_ti = 2 / (1/precisionRate_ti + 1/recallRate_ti)\n",
    "\n",
    "print('precisionRate_ti:', precisionRate_ti)  \n",
    "print('recallRate_ti:', recallRate_ti)        \n",
    "print('F_ti:', F_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisionRate_ti: 0.7329974811083123\n",
      "recallRate_ti: 0.8818181818181818\n",
      "F_ti: 0.8005502063273727\n"
     ]
    }
   ],
   "source": [
    "recallTotal_ti = 0\n",
    "precisionTotal_ti = 0\n",
    "correctTotal_ti = 0\n",
    "\n",
    "with open(\"./resultOf0UseTraindata1.txt\", \"r\", encoding=\"utf-8\") as output:\n",
    "    for line in output:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            pair = line.split(\" \")\n",
    "            if pair[1] == 'ti':\n",
    "                recallTotal_ti += 1\n",
    "\n",
    "            if pair[2] == 'ti':\n",
    "                precisionTotal_ti += 1\n",
    "\n",
    "            if pair[1] == pair[2] and pair[1]== 'ti':\n",
    "                correctTotal_ti += 1  \n",
    "\n",
    "precisionRate_ti = 1.0 * correctTotal_ti / precisionTotal_ti\n",
    "recallRate_ti = 1.0 * correctTotal_ti / recallTotal_ti\n",
    "F_ti = 2 / (1/precisionRate_ti + 1/recallRate_ti)\n",
    "\n",
    "print('precisionRate_ti:', precisionRate_ti)  \n",
    "print('recallRate_ti:', recallRate_ti)        \n",
    "print('F_ti:', F_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisionRate_ti: 0.714622641509434\n",
      "recallRate_ti: 0.9181818181818182\n",
      "F_ti: 0.803713527851459\n"
     ]
    }
   ],
   "source": [
    "recallTotal_ti = 0\n",
    "precisionTotal_ti = 0\n",
    "correctTotal_ti = 0\n",
    "\n",
    "with open(\"./resultOf0UseTraindata2.txt\", \"r\", encoding=\"utf-8\") as output:\n",
    "    for line in output:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            pair = line.split(\" \")\n",
    "            if pair[1] == 'ti':\n",
    "                recallTotal_ti += 1\n",
    "\n",
    "            if pair[2] == 'ti':\n",
    "                precisionTotal_ti += 1\n",
    "\n",
    "            if pair[1] == pair[2] and pair[1]== 'ti':\n",
    "                correctTotal_ti += 1  \n",
    "\n",
    "precisionRate_ti = 1.0 * correctTotal_ti / precisionTotal_ti\n",
    "recallRate_ti = 1.0 * correctTotal_ti / recallTotal_ti\n",
    "F_ti = 2 / (1/precisionRate_ti + 1/recallRate_ti)\n",
    "\n",
    "print('precisionRate_ti:', precisionRate_ti)  \n",
    "print('recallRate_ti:', recallRate_ti)        \n",
    "print('F_ti:', F_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisionRate_ti: 0.6636971046770601\n",
      "recallRate_ti: 0.9030303030303031\n",
      "F_ti: 0.7650834403080873\n"
     ]
    }
   ],
   "source": [
    "recallTotal_ti = 0\n",
    "precisionTotal_ti = 0\n",
    "correctTotal_ti = 0\n",
    "\n",
    "with open(\"./resultOf0UseTraindata4.txt\", \"r\", encoding=\"utf-8\") as output:\n",
    "    for line in output:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            pair = line.split(\" \")\n",
    "            if pair[1] == 'ti':\n",
    "                recallTotal_ti += 1\n",
    "\n",
    "            if pair[2] == 'ti':\n",
    "                precisionTotal_ti += 1\n",
    "\n",
    "            if pair[1] == pair[2] and pair[1]== 'ti':\n",
    "                correctTotal_ti += 1  \n",
    "\n",
    "precisionRate_ti = 1.0 * correctTotal_ti / precisionTotal_ti\n",
    "recallRate_ti = 1.0 * correctTotal_ti / recallTotal_ti\n",
    "F_ti = 2 / (1/precisionRate_ti + 1/recallRate_ti)\n",
    "\n",
    "print('precisionRate_ti:', precisionRate_ti)  \n",
    "print('recallRate_ti:', recallRate_ti)        \n",
    "print('F_ti:', F_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisionRate_ti: 0.7109144542772862\n",
      "recallRate_ti: 0.7303030303030303\n",
      "F_ti: 0.7204783258594918\n"
     ]
    }
   ],
   "source": [
    "recallTotal_ti = 0\n",
    "precisionTotal_ti = 0\n",
    "correctTotal_ti = 0\n",
    "\n",
    "with open(\"./resultOf0UseCorpus3.txt\", \"r\", encoding=\"utf-8\") as output:\n",
    "    for line in output:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            pair = line.split(\" \")\n",
    "            if pair[1] == 'ti':\n",
    "                recallTotal_ti += 1\n",
    "\n",
    "            if pair[2] == 'ti':\n",
    "                precisionTotal_ti += 1\n",
    "\n",
    "            if pair[1] == pair[2] and pair[1]== 'ti':\n",
    "                correctTotal_ti += 1  \n",
    "\n",
    "precisionRate_ti = 1.0 * correctTotal_ti / precisionTotal_ti\n",
    "recallRate_ti = 1.0 * correctTotal_ti / recallTotal_ti\n",
    "F_ti = 2 / (1/precisionRate_ti + 1/recallRate_ti)\n",
    "\n",
    "print('precisionRate_ti:', precisionRate_ti)  \n",
    "print('recallRate_ti:', recallRate_ti)        \n",
    "print('F_ti:', F_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisionRate_ti: 0.5458534444931987\n",
      "recallRate_ti: 0.8062216461438756\n",
      "F_ti: 0.6509680795395081\n"
     ]
    }
   ],
   "source": [
    "recallTotal_ti = 0\n",
    "precisionTotal_ti = 0\n",
    "correctTotal_ti = 0\n",
    "\n",
    "with open(\"./resultOfAllLinkContentUseCorpus3.txt\", \"r\", encoding=\"utf-8\") as output:\n",
    "    for line in output:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            pair = line.split(\" \")\n",
    "            if pair[1] == 'ti':\n",
    "                recallTotal_ti += 1\n",
    "\n",
    "            if pair[2] == 'ti':\n",
    "                precisionTotal_ti += 1\n",
    "\n",
    "            if pair[1] == pair[2] and pair[1]== 'ti':\n",
    "                correctTotal_ti += 1  \n",
    "\n",
    "precisionRate_ti = 1.0 * correctTotal_ti / precisionTotal_ti\n",
    "recallRate_ti = 1.0 * correctTotal_ti / recallTotal_ti\n",
    "F_ti = 2 / (1/precisionRate_ti + 1/recallRate_ti)\n",
    "\n",
    "print('precisionRate_ti:', precisionRate_ti)  \n",
    "print('recallRate_ti:', recallRate_ti)        \n",
    "print('F_ti:', F_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precisionRate_ti: 0.5475868290482634\n",
      "recallRate_ti: 0.7867790019442644\n",
      "F_ti: 0.6457446808510637\n"
     ]
    }
   ],
   "source": [
    "recallTotal_ti = 0\n",
    "precisionTotal_ti = 0\n",
    "correctTotal_ti = 0\n",
    "\n",
    "with open(\"./resultOfAllLinkContentUseCorpus4.txt\", \"r\", encoding=\"utf-8\") as output:\n",
    "    for line in output:\n",
    "        if line:\n",
    "            line = line.strip()\n",
    "            pair = line.split(\" \")\n",
    "            if pair[1] == 'ti':\n",
    "                recallTotal_ti += 1\n",
    "\n",
    "            if pair[2] == 'ti':\n",
    "                precisionTotal_ti += 1\n",
    "\n",
    "            if pair[1] == pair[2] and pair[1]== 'ti':\n",
    "                correctTotal_ti += 1  \n",
    "\n",
    "precisionRate_ti = 1.0 * correctTotal_ti / precisionTotal_ti\n",
    "recallRate_ti = 1.0 * correctTotal_ti / recallTotal_ti\n",
    "F_ti = 2 / (1/precisionRate_ti + 1/recallRate_ti)\n",
    "\n",
    "print('precisionRate_ti:', precisionRate_ti)  \n",
    "print('recallRate_ti:', recallRate_ti)        \n",
    "print('F_ti:', F_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
